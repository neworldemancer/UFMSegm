{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Copyright 2015-2023, University of Bern, Laboratory for High Energy Physics and Theodor Kocher Institute, M. Vladymyrov\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. libs & utils"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "from PIL.Image import enum\n",
    "\n",
    "sys.path.append('../model training')\n",
    "\n",
    "import skimage\n",
    "from skimage.filters import gaussian\n",
    "from scipy.signal import argrelmin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.imgio as iio\n",
    "import utils.imgio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.CZI_image import CZI_image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display \n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "from aicsimageio import AICSImage\n",
    "import re\n",
    "from PIL import Image\n",
    "from utils import imgio as iio"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "def save_pckl(d, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(d, f, protocol=pickle.DEFAULT_PROTOCOL)\n",
    "def load_pckl(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "def ish(im, vmin=0, vmax=255):\n",
    "    plt.imshow(im, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "def read_mp_tiff(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (str) : path to the images, e.g. `/path/to/stacks/img.png`\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): image, DHWC\n",
    "    \"\"\"\n",
    "    img = Image.open(path)\n",
    "    images = []\n",
    "    for i in range(img.n_frames):\n",
    "        img.seek(i)\n",
    "        images.append(np.array(img))\n",
    "    return np.array(images)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def minmax(a):\n",
    "    return np.min(a), np.max(a)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Interface"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1. CZI_image"
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "source": [
    "def save_as_8bit_tifs(root_dir, ds_name, ci:CZI_image):\n",
    "    # format : <ds_name>_t0xcxmx.tif\n",
    "    \n",
    "    n_c = ci.n_c\n",
    "    n_tile = ci.n_tile\n",
    "    n_t = ci.n_t\n",
    "    \n",
    "    \n",
    "    tmpl_t = 't%0' + '%d' % len('%d' % n_t) + 'd'\n",
    "    tmpl_ch = 'c%0' + '%d' % len('%d' % n_c) + 'd'\n",
    "    tmpl_tl = 'm%0' + '%d' % len('%d' % n_tile) + 'd'\n",
    "    \n",
    "    path = os.path.join(root_dir, ds_name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    for ch in range(n_c):\n",
    "        arr = [minmax(ci.get_frame(tile=tile, t=t, c=ch)) for t in range(n_t) for tile in range(n_tile)]\n",
    "        arr = np.array(arr).T\n",
    "        v_min = np.min(arr[0])\n",
    "        v_max = np.max(arr[1])\n",
    "        del arr\n",
    "        \n",
    "        print(ch, v_min, v_max)\n",
    "        \n",
    "        for t in tqdm(range(n_t)):\n",
    "            for tile in range(n_tile):\n",
    "        \n",
    "                sfx = '_'\n",
    "                sfx += tmpl_t % (t+1)\n",
    "\n",
    "                sfx += tmpl_ch % (ch+1)\n",
    "\n",
    "                sfx += tmpl_tl % (tile+1)\n",
    "\n",
    "                name = os.path.join(path, ds_name + sfx + '.tif')\n",
    "                \n",
    "                im = ci.get_frame(tile=tile, t=t, c=ch)\n",
    "                im_normed = (im-v_min) / (v_max - v_min)\n",
    "                im_8b = (255*im_normed.clip(0, 1)).astype(np.uint8)\n",
    "                \n",
    "                iio.save_image(im_8b, name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "source": [
    "def dataset_from_czi(root_dir, image_file_name):\n",
    "    \"\"\"\n",
    "    Generates images from tiles\n",
    "    \"\"\"\n",
    "    # read\n",
    "    print('reading')\n",
    "    ci = CZI_image(image_file_name)\n",
    "    ds_name = os.path.splitext(os.path.basename(image_file_name))[0]\n",
    "    save_as_8bit_tifs(root_dir, ds_name, ci)\n",
    "    del ci"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## splitting tiles:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_ds_dir(idx, ds_name, datasets_path):\n",
    "    path = os.path.join(datasets_path, '%03d'%idx)\n",
    "    if os.path.exists(path):\n",
    "        return False\n",
    "    else:\n",
    "        path = os.path.join(path, ds_name)\n",
    "        os.makedirs(path, exist_ok=False)\n",
    "        return True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_record_info(ds_inf_file_path, idx, ds_name, datasets_path, tile = None):\n",
    "    ttl = ds_name + f', tile {tile}' if tile is not None else ''\n",
    "    \n",
    "    ds_inf = read_info_file(ds_inf_file_path)\n",
    "    save_ds_inf(ds_inf_file_path, ds_inf, [ttl])\n",
    "    ds_inf = read_info_file(ds_inf_file_path)\n",
    "    \n",
    "    assert max(ds_inf.keys()) == idx, f'max({ds_inf.keys()}) ={max(ds_inf.keys())} != {idx}'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ds_info(ds_path):\n",
    "    \n",
    "    list_files = sorted([n for n in os.listdir(ds_path) if '.tif' in n])\n",
    "    \n",
    "    \n",
    "    file_names = [fn.replace('.tif', '') for fn in list_files if ('t' in fn and '_' in fn)] # only formatted, timelapse\n",
    "    \n",
    "    list_sfx = [fn.split('_')[-1] for fn in file_names]\n",
    "    \n",
    "    name_tmpls = ['_'.join(fn.split('_')[:-1]) for fn in file_names if '_' in fn]\n",
    "    \n",
    "    if len(name_tmpls) == 0:\n",
    "        raise FileNotFoundError('files with name \"<xxx>_<xx>t%d<xx>.tif not found\"')\n",
    "    \n",
    "    name_tmpl = name_tmpls[0]\n",
    "    \n",
    "    last_sfx = list_sfx[-1]\n",
    "    has_ch =  'c' in last_sfx\n",
    "    has_tl =  'm' in last_sfx\n",
    "    has_ps =  's' in last_sfx  # positions\n",
    "    \n",
    "    if has_ps:\n",
    "        last_sfx_trunc_s = last_sfx.replace('s', '')\n",
    "        \n",
    "        n_ps_s, res_sfx = last_sfx_trunc_s.split('t')\n",
    "        n_ps = int(n_ps_s)\n",
    "        \n",
    "        if has_ch:\n",
    "            n_t_s, ch_tl_s = res_sfx.split('c')\n",
    "            n_t = int(n_t_s)\n",
    "\n",
    "            if has_tl:\n",
    "                n_ch, n_tl = [int(s) for s in ch_tl_s.split('m')]\n",
    "            else:\n",
    "                n_ch = int(ch_tl_s)\n",
    "                n_tl = 1\n",
    "        else:\n",
    "            if has_tl:\n",
    "                n_t, n_tl = [int(s) for s in res_sfx.split('m')]\n",
    "            else:\n",
    "                n_t = int(res_sfx)\n",
    "                n_tl = 1\n",
    "            n_ch = 1\n",
    "    else:\n",
    "        n_ps = 1\n",
    "        last_sfx_trunc_t = last_sfx.replace('t', '')\n",
    "        if has_ch:\n",
    "            n_t_s, ch_tl_s = last_sfx_trunc_t.split('c')\n",
    "            n_t = int(n_t_s)\n",
    "\n",
    "            if has_tl:\n",
    "                n_ch, n_tl = [int(s) for s in ch_tl_s.split('m')]\n",
    "            else:\n",
    "                n_ch = int(ch_tl_s)\n",
    "                n_tl = 1\n",
    "        else:\n",
    "            if has_tl:\n",
    "                n_t, n_tl = [int(s) for s in last_sfx_trunc_t.split('m')]\n",
    "            else:\n",
    "                n_t = int(last_sfx_trunc_t)\n",
    "                n_tl = 1\n",
    "            n_ch = 1\n",
    "        \n",
    "    \n",
    "    tmpl_ps = 's%0' + '%d' % len('%d' % n_tl) + 'd'\n",
    "    tmpl_t = 't%0' + '%d' % len('%d' % n_t) + 'd'\n",
    "    tmpl_ch = 'c%0' + '%d' % len('%d' % n_ch) + 'd'\n",
    "    tmpl_tl = 'm%0' + '%d' % len('%d' % n_tl) + 'd'\n",
    "    \n",
    "    return {'n_ps' : n_ps, \n",
    "            'n_t' : n_t, \n",
    "            'n_ch' :n_ch, \n",
    "            'n_tl' : n_tl, \n",
    "            'has_ps' :has_ps, \n",
    "            'has_ch' :has_ch, \n",
    "            'has_tl' :has_tl,\n",
    "            'tmpl_ps' :tmpl_ps,\n",
    "            'tmpl_t' :tmpl_t,\n",
    "            'tmpl_ch' :tmpl_ch,\n",
    "            'tmpl_tl' :tmpl_tl,\n",
    "            'name_tmpl':name_tmpl\n",
    "           }\n",
    "\n",
    "def get_file_name(path, inf_dict, ps, t, ch, tile):\n",
    "    sfx = '_'\n",
    "    \n",
    "    if inf_dict['has_ps']:\n",
    "        sfx += inf_dict['tmpl_ps'] % (ps+1)\n",
    "    sfx += inf_dict['tmpl_t'] % (t+1)\n",
    "    if inf_dict['has_ch']:\n",
    "        sfx += inf_dict['tmpl_ch'] % (ch+1)\n",
    "    if inf_dict['has_tl']:\n",
    "        sfx += inf_dict['tmpl_tl'] % (tile+1)\n",
    "\n",
    "    name = inf_dict['name_tmpl'] + sfx + '.tif'\n",
    "    fn = os.path.join(path, name)\n",
    "    return fn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_segmentation_datasets(datasets_path, datasets_names, start_ds_idx, ds_inf_file_path, time_subsample_min_t = 210):  # if n_t >time_subsample_min_t - take every second frame\n",
    "    for ds_names in datasets_names:\n",
    "        if isinstance(ds_names, str):\n",
    "            ds_names = [(0, ds_names, 0)]\n",
    "            \n",
    "        # 1. get info for all\n",
    "        copy_struct = []  # 1 element per ds: (ds_dir_name, info, n_before, n_after)\n",
    "        \n",
    "        all_nt = []\n",
    "        for item in ds_names:\n",
    "            n_copy_before, ds_path, n_copy_after = item\n",
    "            inf = ds_info(ds_path)\n",
    "            copy_struct.append((ds_path, inf, n_copy_before, n_copy_after))\n",
    "            all_nt.append(inf['n_t'])\n",
    "            \n",
    "        subsample_fact = 2 if max(all_nt)>time_subsample_min_t else 1\n",
    "        # print(subsample_fact, copy_struct, '\\n')\n",
    "        # continue\n",
    "            \n",
    "        # check validity: all mush have same format\n",
    "        for key in ['n_ps', 'n_ch', 'n_tl', 'has_ps', 'has_ch', 'has_tl']:\n",
    "            el0 = copy_struct[0][1][key]\n",
    "            for struct in copy_struct[1:]:\n",
    "                assert(struct[1][key] == el0)\n",
    "        \n",
    "        n_t_out = 0  # num output timeframes\n",
    "        for struct in copy_struct:\n",
    "            n_t_out += struct[2] + struct[3] + (struct[1]['n_t'] // subsample_fact)\n",
    "            \n",
    "        inf0 = copy_struct[0][1]\n",
    "        n_ch = inf0['n_ch']\n",
    "        \n",
    "        n_ps_i = inf0['n_ps']\n",
    "        n_tl_i = inf0['n_tl']\n",
    "        \n",
    "        n_tl = n_ps_i * n_tl_i\n",
    "        \n",
    "        has_ch = inf0['has_ch']\n",
    "        has_tl = inf0['has_ps'] or inf0['has_tl']\n",
    "        \n",
    "        out_ds_name_general = '_'.join([struct[1]['name_tmpl'] for struct in copy_struct])\n",
    "        \n",
    "        tmpl_t = 't%0' + '%d' % len('%d' % n_t_out) + 'd'\n",
    "        tmpl_ch = 'c%0' + '%d' % len('%d' % n_ch) + 'd'\n",
    "        tmpl_tl = 'm%0' + '%d' % len('%d' % 1) + 'd'\n",
    "        \n",
    "        oinf = {'n_ps' : 1, \n",
    "                'n_t' : n_t_out, \n",
    "                'n_ch' :n_ch, \n",
    "                'n_tl' : 1, \n",
    "                'has_ps' :False,\n",
    "                'has_ch' :has_ch, \n",
    "                'has_tl' :False,\n",
    "                'tmpl_t' :tmpl_t,\n",
    "                'tmpl_ch' :tmpl_ch,\n",
    "                'tmpl_tl' :tmpl_tl,\n",
    "                'name_tmpl':out_ds_name_general\n",
    "               }\n",
    "        \n",
    "        # create dirs and fill info file\n",
    "        tile_to_idx = {}\n",
    "        tile_ds_name = {}\n",
    "        \n",
    "        \n",
    "        creation_ok = True\n",
    "        for tl in range(n_tl):\n",
    "            idx = start_ds_idx + tl\n",
    "            tile_to_idx[tl] = idx\n",
    "    \n",
    "            out_ds_name = out_ds_name_general + ('_tile%d' % (tl+1) if has_tl else '')\n",
    "            tile_ds_name[tl] = out_ds_name\n",
    "    \n",
    "            if not create_ds_dir(idx, out_ds_name, datasets_path):\n",
    "                print('dataset with idx', idx, 'already exists. please check manually. Aborting.')\n",
    "                creation_ok = False\n",
    "                break\n",
    "        if not creation_ok:\n",
    "            break\n",
    "            \n",
    "        start_ds_idx += n_tl\n",
    "    \n",
    "    \n",
    "        for tl in range(n_tl):\n",
    "            idx = tile_to_idx[tl]\n",
    "            out_ds_name = tile_ds_name[tl]\n",
    "            make_record_info(ds_inf_file_path, idx, out_ds_name, datasets_path, (tl+1) if has_tl else None)\n",
    "    \n",
    "            ods_path = os.path.join(datasets_path, '%03d' % idx)\n",
    "            with open(os.path.join(ods_path, 'info.txt'), 'wt') as f:\n",
    "                f.write(out_ds_name)\n",
    "    \n",
    "        \n",
    "        for ps_i in range(n_ps_i):\n",
    "            for tl_i in range(n_tl_i):\n",
    "                tl = ps_i * n_tl_i + tl_i\n",
    "                \n",
    "                idx = tile_to_idx[tl]\n",
    "                out_ds_name = tile_ds_name[tl]\n",
    "                oinf['name_tmpl'] = out_ds_name\n",
    "    \n",
    "                ods_path = os.path.join(datasets_path, '%03d' % idx, out_ds_name)\n",
    "                \n",
    "                block_boundaries = []\n",
    "                for ch in range(n_ch):\n",
    "                    t_o = 0\n",
    "                    for struct in copy_struct:\n",
    "                        in_path, inf, copy_before, copy_after = struct\n",
    "                        n_t_i = inf['n_t']\n",
    "                        for i in range(copy_before):\n",
    "                            t_i = 0\n",
    "                            i_file = get_file_name(in_path,   inf, ps_i, t_i, ch, tl_i)\n",
    "                            o_file = get_file_name(ods_path, oinf,   -1, t_o, ch, 0)\n",
    "                            shutil.copy(i_file, o_file)\n",
    "                            t_o += 1\n",
    "    \n",
    "                        for t_i in range(n_t_i//subsample_fact):\n",
    "                            i_file = get_file_name(in_path,   inf, ps_i, t_i*subsample_fact, ch, tl_i)\n",
    "                            o_file = get_file_name(ods_path, oinf,   -1, t_o, ch, 0)\n",
    "                            shutil.move(i_file, o_file)\n",
    "                            t_o += 1\n",
    "    \n",
    "                        for i in range(copy_after):\n",
    "                            i_file = get_file_name(ods_path, oinf, ps_i, t_o-1, ch, tl_i)\n",
    "                            o_file = get_file_name(ods_path, oinf,   -1, t_o,   ch, 0)\n",
    "                            shutil.copy(i_file, o_file)\n",
    "                            t_o += 1\n",
    "                            \n",
    "                        if ch==0:\n",
    "                            begin = 0 if len(block_boundaries)==0 else block_boundaries[-1][1]\n",
    "                            end = t_o\n",
    "                            block_boundaries.append([begin, end])\n",
    "                            \n",
    "                block_info_path = os.path.join(datasets_path, '%03d' % idx, 'block_info.txt')\n",
    "                with open(block_info_path, 'wt') as f:\n",
    "                    txt = '|'.join([' '.join([str(bi) for bi in b]) for b in block_boundaries])\n",
    "                    f.write(txt)\n",
    "    \n",
    "        for item in ds_names:\n",
    "            n_copy_before, ds_path, n_copy_after = item\n",
    "            #shutil.rmtree(ds_path)\n",
    "    \n",
    "    return start_ds_idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# #test:\n",
    "# \n",
    "# fname = r'Untreated.czi'\n",
    "# root_dir = os.path.dirname(fname)\n",
    "# ds_name = os.path.splitext(os.path.basename(fname))[0]\n",
    "# \n",
    "# \n",
    "# ci = CZI_image(file_name=fname)\n",
    "# save_as_8bit_tifs(root_dir, ds_name, ci)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. batch proc"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "src_dir = r'D:\\data\\set\\path'\n",
    "root_dir = src_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "all_ds = list(glob.glob(src_dir+ '\\\\**\\\\'+'*.czi', recursive=True))\n",
    "for name in all_ds:\n",
    "    print(name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# %%time\n",
    "# \n",
    "# for fname in all_ds:\n",
    "#     dataset_from_czi(root_dir, fname)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Ipywidgets interface"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ipywidgets interface:\n",
    "# 1. Files path input string \"CZI Datasets path\"\n",
    "# 2. Directory path input string \"Segmentation datasets path\"\n",
    "# 3. File path with datasets IDs list - text human readable file with dataset id - names pairs like \" 85 - Untreated_2024.06.18\" \n",
    "# 4. Button \"Process\"\n",
    "\n",
    "# 5. Output - text box with the progress\n",
    "\n",
    "# Make interface:\n",
    "\n",
    "@dataclass\n",
    "class  DataImportConfig:\n",
    "    raw_ds_path: str = os.path.abspath('../../datasets_raw')\n",
    "    seg_ds_path: str = os.path.abspath('../../datasets_seg')\n",
    "    ds_inf_path: str = os.path.abspath('../../datasets_seg/info.txt') \n",
    "    \n",
    "# methods for loading and storing the config to file\n",
    "_cfg_filename = 'import_cfg.json'\n",
    "\n",
    "\n",
    "def load_import_cfg(cfg_path=None):\n",
    "    cfg_path = cfg_path or os.path.join(os.path.abspath(os.path.curdir), _cfg_filename)\n",
    "    if os.path.exists(cfg_path):\n",
    "        with open(cfg_path, 'rt') as f:\n",
    "            cfg = DataImportConfig(**json.load(f))\n",
    "    else:\n",
    "        cfg = DataImportConfig()\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def save_import_cfg(cfg, cfg_path=None):\n",
    "    cfg_path = cfg_path or os.path.join(os.path.abspath(os.path.curdir), _cfg_filename)\n",
    "    with open(cfg_path, 'wt') as f:\n",
    "        # human-readable, 4 spaces indentation\n",
    "        json.dump(asdict(cfg), f, indent=4)\n",
    "        \n",
    "\n",
    "import_cfg = load_import_cfg()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Files path input string \"CZI Datasets path\"\n",
    "raw_ds_path = widgets.Text(value=import_cfg.raw_ds_path, description='CZI Datasets path:', disabled=False)\n",
    "\n",
    "# 2. Directory path input string \"Segmentation datasets path\"\n",
    "seg_ds_path = widgets.Text(value=import_cfg.seg_ds_path, description='Segmentation datasets path:', disabled=False)\n",
    "\n",
    "# 3. File path with datasets IDs list - text human readable file with dataset id - names pairs like \" 85 - Untreated_2024.06.18\"\n",
    "ds_inf_path = widgets.Text(value=import_cfg.ds_inf_path, description='Datasets info path:', disabled=False)\n",
    "\n",
    "# 4. Button \"Process\"\n",
    "process_btn = widgets.Button(description='Process')\n",
    "\n",
    "# 5. Output - text box with the progress\n",
    "out = widgets.Output()\n",
    "\n",
    "def read_info_file(ds_inf_file_path):\n",
    "    try:\n",
    "        with open(ds_inf_file_path, 'rt') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        lines = []\n",
    "    ds_inf = {}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        subs = line.split('-')\n",
    "        k = int(subs[0].strip())\n",
    "        v = '-'.join(subs[1:])\n",
    "        \n",
    "        ds_inf[k] = v.strip()\n",
    "    return ds_inf\n",
    "\n",
    "def save_ds_inf(ds_inf_file_path, ds_inf, new_ds_list):\n",
    "    with open(ds_inf_file_path, 'wt') as f:\n",
    "        for k, v in ds_inf.items():\n",
    "            f.write(f'{k} - {v}\\n')\n",
    "            \n",
    "        next_idx = (max(ds_inf.keys())+1) if len(ds_inf) else 0\n",
    "        for i, title in enumerate(new_ds_list):\n",
    "            f.write(f'{next_idx+i} - {title}\\n')\n",
    "            \n",
    "def on_process_click(b):\n",
    "    with out:\n",
    "        try:\n",
    "            print('Processing...')\n",
    "            # save config\n",
    "            import_cfg.raw_ds_path = raw_ds_path.value\n",
    "            import_cfg.seg_ds_path = seg_ds_path.value\n",
    "            import_cfg.ds_inf_path = ds_inf_path.value\n",
    "            \n",
    "            save_import_cfg(import_cfg)\n",
    "            print('Saved cfg')\n",
    "            \n",
    "            ds_inf = read_info_file(import_cfg.ds_inf_path)\n",
    "            \n",
    "            print('Read info')\n",
    "            start_ds_idx = (max(ds_inf.keys())+1) if len(ds_inf) else 0\n",
    "            print(f'start_ds_idx = {start_ds_idx}')\n",
    "            # process\n",
    "            all_ds = list(glob.glob(import_cfg.raw_ds_path+ '\\\\**\\\\'+'*.czi', recursive=True))\n",
    "            for name in all_ds:\n",
    "                print(name)\n",
    "            for fname in all_ds:\n",
    "                dataset_from_czi(import_cfg.raw_ds_path, fname)\n",
    "            print('Done')\n",
    "            \n",
    "            datasets_names = []\n",
    "            path = import_cfg.raw_ds_path\n",
    "            path = os.path.abspath(path)\n",
    "            for p2 in sorted(os.listdir(path)):\n",
    "                    path3 = os.path.join(path, p2)\n",
    "                    if not os.path.isdir(path3):\n",
    "                        continue\n",
    "                    datasets_names.append([[0, path3, 0]])\n",
    "                \n",
    "            print(f'datasets_names = {datasets_names}')\n",
    "            start_ds_idx = create_segmentation_datasets(import_cfg.seg_ds_path,\n",
    "                                                        datasets_names,\n",
    "                                                        start_ds_idx,\n",
    "                                                        import_cfg.ds_inf_path\n",
    "                                                        )\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            trace_str = traceback.format_exc()\n",
    "            print(trace_str, flush=True)\n",
    "        \n",
    "        \n",
    "process_btn.on_click(on_process_click)\n",
    "\n",
    "vbox = widgets.VBox([raw_ds_path, seg_ds_path, ds_inf_path, process_btn, out])\n",
    "display(vbox)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
