{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of dataset XXX by XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "sys.path.append('../model training')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import interp1d as interp\n",
    "\n",
    "from utils import net_utils as nu\n",
    "from utils import imgio as iio\n",
    "from utils import predictor as pr\n",
    "from utils.histnorm import NormHist\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time as timer\n",
    "from time import sleep\n",
    "import shutil\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "from threading import Thread\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_start_t = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_ids = list(range(281, 289)) # range of tiled datasets, expected to be last folders, and 8 tiles\n",
    "merged_ds_idx = datasets_ids[-1] +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#tiles_info\n",
    "tiles_info = {} #id: [[ny, nx], list_of_tile_dataset_ids],\n",
    "ny, nx = 2, 4\n",
    "n_tiles = ny*nx\n",
    "\n",
    "\n",
    "for idx_ofs, tiles_start_idx in enumerate(datasets_ids[::n_tiles]):\n",
    "    tiles_info[merged_ds_idx+idx_ofs] = [[ny, nx], list(range(tiles_start_idx, tiles_start_idx+n_tiles))]\n",
    "print('check if the ids are correct CAREFULLY! Type Y if correct.')\n",
    "print(tiles_info)\n",
    "a = input()\n",
    "if a.lower()!='y':\n",
    "    raise ValueError(\"Process terminated by user's request: tiles info is NOT correct.\")\n",
    "\n",
    "overlap = 1389 * 0.12  # 12% of width\n",
    "tile_dx = int(1389 - overlap)\n",
    "tile_dy = int(1040 - overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = '../datasets/'\n",
    "datasets_path_proc = 'Q:\\\\' # path from processing node. Should be consistent with cfg files, should be within closed privvate network\n",
    "\n",
    "proc_bin_path = 'C:\\\\VivoFollow\\\\bin\\\\'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_ids = [0, 1, 2, 3, 4, 5, 6, 7] # list of available GPUs\n",
    "dev_id = dev_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "auto_proc = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fill datasets info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rds_id = 7 # ref. ds. id\n",
    "\n",
    "nums=[]\n",
    "datasets_names = []\n",
    "fluo_present = []\n",
    "\n",
    "datasets_tmplts = []\n",
    "datasetsf_tmplts = []\n",
    "datasets_normed = []\n",
    "datasets_hist_normed = []\n",
    "\n",
    "block_boundaries = []\n",
    "\n",
    "for ds_idx in datasets_ids:\n",
    "    # read info file\n",
    "    info_file_name = datasets_path + '%d/info.txt' % ds_idx\n",
    "    with open(info_file_name, 'rt') as f:\n",
    "        ds_name = f.readline()\n",
    "    \n",
    "    #list files in dir\n",
    "    ds_path = datasets_path + '%d/%s/' % (ds_idx, ds_name)\n",
    "    all_tif_files = [n for n in os.listdir(ds_path) if ds_name in n]\n",
    "    \n",
    "    all_sfx = sorted([n.replace(ds_name+'_t', '').replace('.tif', '') for n in all_tif_files])\n",
    "    last_sfx = all_sfx[-1]\n",
    "    \n",
    "    #get # channel, #time points\n",
    "    has_fluo = 'c' in last_sfx\n",
    "    \n",
    "    if has_fluo:\n",
    "        n_t_c_s = last_sfx.split('c')\n",
    "        n_t, n_c = [int(s) for s in n_t_c_s]\n",
    "    else:\n",
    "        n_c = 1\n",
    "        n_t = int(last_sfx)\n",
    "    \n",
    "    #fill tmplts, num, fluo present, \n",
    "    tmpl_t = '_t%0' + '%d' % len('%d' % n_t) + 'd'\n",
    "    tmpl_c = 'c%0' + '%d' % len('%d' % n_c if has_fluo else '1') + 'd'\n",
    "    \n",
    "    dataset_tmplt = ds_path + ds_name + tmpl_t + (tmpl_c % 1 if has_fluo else '') +'.tif'\n",
    "    datasetf_tmplts = [ds_path + ds_name + tmpl_t + (tmpl_c % ch) +'.tif' for ch in range(2, n_c+1)] if has_fluo else []\n",
    "    dataset_normed = ds_path + 'normed'\n",
    "    \n",
    "    dataset_hist_normed =  datasets_path + '%d/' % ds_idx + 'hist_normed/img_%03d.tif' #png\n",
    "\n",
    "    #print(n_t, n_c, has_fluo, tmpl_t, tmpl_c, dataset_tmplt, datasetf_tmplts, dataset_normed)\n",
    "    \n",
    "    # use num per dataset\n",
    "    nums.append(n_t)\n",
    "    datasets_names.append(ds_name)\n",
    "    fluo_present.append(has_fluo)\n",
    "\n",
    "    datasets_tmplts.append(dataset_tmplt)\n",
    "    datasetsf_tmplts.append(datasetf_tmplts)\n",
    "    datasets_normed.append(dataset_normed)\n",
    "    \n",
    "    datasets_hist_normed.append(dataset_hist_normed)\n",
    "    \n",
    "\n",
    "    block_info_path = os.path.join(datasets_path, '%d' % ds_idx, 'block_info.txt')\n",
    "    if os.path.exists(block_info_path):\n",
    "        with open(block_info_path, 'rt') as f:\n",
    "            txt = f.readline()\n",
    "    else:\n",
    "        txt = ''\n",
    "    if txt:\n",
    "        block_boundary = [[int(bi) for bi in b.split(' ')] for b in txt.split('|')]\n",
    "    else:\n",
    "        block_boundary = [[0, n_t]]\n",
    "    block_boundaries.append(block_boundary)\n",
    "    \n",
    "al_datasets_tmplts = [datasets_path +'%d'%i+ '/imgs_aligned_all/raw/%03d.png' for i,n in zip(datasets_ids,datasets_names)]\n",
    "al_datasets_normed = [datasets_path +'%d'%i+ '/imgs_aligned_all/normed' for i,n in zip(datasets_ids,datasets_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## MT predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     20,
     23,
     33
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PredictorMT:\n",
    "    def __init__(self, mod_path, mod_itr,\n",
    "                 gpu_ids,\n",
    "                 batch_sz, input_sz,\n",
    "                 io_map,\n",
    "                 z_border=2  # one side\n",
    "                ):\n",
    "        self.z_border = z_border\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.n_thr = len(self.gpu_ids)\n",
    "        \n",
    "        self.predictors = [pr.Predictor(mod_path, mod_itr,\n",
    "                                        device_id=None,\n",
    "                                        device_ids=gpu_ids,\n",
    "                                        gpuid=i,\n",
    "                                        batch_sz=batch_sz, input_sz=input_sz,\n",
    "                                        in_out_dict=io_map,\n",
    "                                       ) for i, dev_id in enumerate(gpu_ids)]\n",
    "        self.res = []\n",
    "        \n",
    "    def normalize_stack(self, stack, norm_stack, normalization_percentile_range):\n",
    "        return self.predictors[0].normalize_stack(stack, norm_stack, normalization_percentile_range)\n",
    "    \n",
    "    def split_stack(self, stack):\n",
    "        n_z = len(stack)\n",
    "        n_z_chunk = (n_z + self.n_thr - 1) // self.n_thr\n",
    "        \n",
    "        begin_end_pairs = [[max(0, i*n_z_chunk-self.z_border), \n",
    "                            min(n_z, (i+1)*n_z_chunk+self.z_border)] for i in range(self.n_thr)]\n",
    "        \n",
    "        chunks = [stack[b:e] for b,e in begin_end_pairs]\n",
    "        return chunks\n",
    "    \n",
    "    def merge_result(self, stacks):\n",
    "        #self.z_border\n",
    "        \n",
    "        cropped_overlap = [\n",
    "                            s[0 if i==0 else self.z_border:\n",
    "                              len(s) if i==(self.n_thr-1) else -self.z_border\n",
    "                             ]\n",
    "                            for i, s in enumerate(stacks)\n",
    "                          ]\n",
    "        \n",
    "        res = np.concatenate(cropped_overlap, axis=0)\n",
    "        return res\n",
    "    \n",
    "    def _predict_image_stack(self, stack_norm, margin, keep_edge, edge_size, thr_idx):\n",
    "        pred = self.predictors[thr_idx]\n",
    "        res = pred.predict_image_stack(stack_norm, margin, keep_edge, edge_size)\n",
    "        self.res[thr_idx] = res\n",
    "        \n",
    "    def predict_image_stack(self, stack_norm, margin, keep_edge=True, edge_size=0):\n",
    "        # split stack\n",
    "        chunks = self.split_stack(stack_norm)\n",
    "        \n",
    "        self.res = [None for i in range(self.n_thr)]\n",
    "        \n",
    "        #make threads\n",
    "        threads = [Thread(target = self._predict_image_stack,\n",
    "                          args = (chunks[i], margin, keep_edge, edge_size, i)\n",
    "                         ) for i in range(self.n_thr)]\n",
    "        \n",
    "        #run all\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        \n",
    "        #wait all\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        merged = self.merge_result(self.res)\n",
    "        #merge ouput\n",
    "        #processed.shape == (192, 1077, 1405, 3), i.e. plain np tensor\n",
    "        \n",
    "        return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Save constatnt models for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load models\n",
    "# fast model\n",
    "#mod_path = '../BBB/model_BBB_BN_TV_FCN4_CDC_2D_2019.10.21_14-49/'  # 2D, 5 tf\n",
    "#mod_itr = 80001\n",
    "\n",
    "#mod_path = '../BBB/model_BBB_BN_TV_FCN4_CDC_2D_2019.11.11_01-11/'  # 2D, 1 tf (last prod)\n",
    "#mod_itr = 80001\n",
    "\n",
    "#mod_path = '../BBB/model_BBB_BN_TV_FCN4_CDC_2D_2019.12.03_19-09/'  # 2D, 1 tf, with new BN (never used)\n",
    "#mod_itr = 80001\n",
    "\n",
    "mod_path = '../BBB/model_BBB_BN_TV_FCN4_HN_CDC_2D_2021.04.17_22-54/'  # 2D, 1 tf 2021 best \n",
    "mod_itr = 30001\n",
    "\n",
    "io_map = {'in':'stack:0', 'out':'ModelOutput:0', 'out_channels':[0,1,2]}\n",
    "batch_sz=8\n",
    "\n",
    "print('AFTER EXECUTING THIS FIRST TIME FOR A MODEL (Needs single-threaded version!) - RESTART THE KERNEL')\n",
    "print(mod_path, mod_itr)\n",
    "# pred_fast = pr.Predictor(mod_path, mod_itr,\n",
    "#                          device_id=dev_id,\n",
    "#                          batch_sz=batch_sz, input_sz=[512,512],\n",
    "#                          in_out_dict=io_map)\n",
    "\n",
    "pred_fast_mt = PredictorMT(mod_path, mod_itr,\n",
    "                   gpu_ids=dev_ids,\n",
    "                   batch_sz=batch_sz, input_sz=[512,512],\n",
    "                   io_map=io_map)\n",
    "\n",
    "#mod_path = '../BBB/model_BBB_BN_TV_FCNx16_CDC_2019.10.22_18-55/'\n",
    "#mod_path = '../BBB/model_BBB_BN_TV_FCNx16_CDC_2019.10.17_17-50/'\n",
    "#mod_itr = 120001\n",
    "\n",
    "# mod_path = '../BBB/model_BBB_BN_TV_FCNx16_CDC_2020.01.25_19-22/'  # last prod\n",
    "# mod_itr = 100001\n",
    "\n",
    "mod_path = '../BBB/model_BBB_BN_TV_FCNx16_HN_CDC_2021.07.06_16-25/'  # 3D, 2021 best\n",
    "mod_itr = 35001\n",
    "\n",
    "io_map = {'in':'stack:0', 'out':'ModelOutput:0', 'out_channels':[0,1,2]}\n",
    "\n",
    "batch_sz=1\n",
    "\n",
    "print(mod_path, mod_itr)\n",
    "# pred = pr.Predictor(mod_path, mod_itr,\n",
    "#                     device_id=dev_id,\n",
    "#                     batch_sz=batch_sz, input_sz=[512,512],\n",
    "#                     in_out_dict=io_map)\n",
    "pred_mt = PredictorMT(mod_path, mod_itr,\n",
    "                   gpu_ids=dev_ids,\n",
    "                   batch_sz=batch_sz, input_sz=[512,512],\n",
    "                   io_map=io_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load datasets, save match size, normalize and save normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for tmpl, nnrm, num in zip(datasets_tmplts, datasets_normed, nums):\n",
    "    print(tmpl)\n",
    "    stack = iio.read_image_stack(tmpl, num, 1)\n",
    "    \n",
    "    print('saving...')\n",
    "    if len(stack.shape)==4:\n",
    "        stack = stack[...,0]\n",
    "    \n",
    "    for idx, im in enumerate(stack):\n",
    "        iio.save_image(im, tmpl % (idx+1))\n",
    "    \n",
    "    stack_center = stack[:, 130:-130, 130:-130]\n",
    "    \n",
    "    print('normalizing...')\n",
    "    stack_norm = pr.Predictor.normalize_stack(stack=stack, \n",
    "                                              norm_stack=stack_center, \n",
    "                                              normalization_percentile_range=(2.5, 97.5))\n",
    "    print('saving normalized...')\n",
    "    np.savez(nnrm, stack_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generate cell probability maps for alignment and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def proc_path(path):\n",
    "    return path.replace(datasets_path, datasets_path_proc).replace('/','\\\\').replace('%','%%')\n",
    "\n",
    "def make_genmask_bat(ds_idx, stck_tmpl, num):\n",
    "    # gen background mask generation bat file. Alignment pipeline is used\n",
    "    prog = proc_bin_path+'TimeFramesAligner_64.exe '\n",
    "    cfg = f'-cfg:{proc_bin_path}TFAligner_bg_mask.cfg '\n",
    "    \n",
    "\n",
    "    mask_tmpl = datasets_path +'%d/'%ds_idx+'pred_for_algn/'+'img_%03d.png'\n",
    "    tgt_dir = datasets_path +'%d/'%ds_idx+'imgs_aligned_all/'\n",
    "    s_t = proc_path(stck_tmpl)\n",
    "    m_t = proc_path(mask_tmpl)\n",
    "    tgd = proc_path(tgt_dir)\n",
    "    \n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd %~dp0\\n'\n",
    "    cmd += prog+cfg\n",
    "    cmd += '-savedir:\"%s\" ' % tgd\n",
    "    cmd += '-n_itr:0 '\n",
    "    cmd += '-stack:\"%s\" ' % s_t\n",
    "    cmd += '-mask:\"%s\" ' % m_t\n",
    "    cmd += '-n_frames:%d ' %  num\n",
    "    \n",
    "    cmd += '\\n'\n",
    "    \n",
    "    cmd += 'popd\\n'\n",
    "        \n",
    "    bat_file = datasets_path + '%d/'%ds_idx + 'genmask.bat'\n",
    "    #print(cmd)\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gen_collective_genmask(ds_idx_list):\n",
    "    \"\"\"\n",
    "    Generated all genmask bat file: executes individual genmask\n",
    "    returns: full path to created batfile\n",
    "    \"\"\"\n",
    "    #cmd = '@echo off\\n'\n",
    "    cmd = ''\n",
    "\n",
    "    for ds_idx in ds_idx_list:\n",
    "        cmd += 'call %d'%ds_idx+'\\\\genmask.bat \\n'\n",
    "    bat_file = datasets_path + 'genmask_' + str(ds_idx_list) + '.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)\n",
    "        \n",
    "    return bat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def start_remote_job(datasets_path, bat_file):\n",
    "    itr = 0\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        if itr==0:\n",
    "            print('Waiting previous remote job to be done...')\n",
    "        itr += 1\n",
    "        sleep(5)\n",
    "    shutil.copy(bat_file, datasets_path + 'remote.bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_mdl_smpl(raw, cdc):\n",
    "    mdl=len(cdc)//2\n",
    "    _=iio.draw_samples(\n",
    "        (\n",
    "            raw[mdl,...,0],\n",
    "            cdc[mdl,...,0],\n",
    "            cdc[mdl,...,1],\n",
    "            cdc[mdl,...,2]\n",
    "        ), color_range=(0,256)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t0 = timer()\n",
    "for i, (nnrm, num, ds_idx, stck_tmpl) in enumerate(zip(datasets_normed, nums, datasets_ids, datasets_tmplts)):\n",
    "    # read normalized\n",
    "    stack_normf = np.load(nnrm+'.npz')\n",
    "    for stack_norm in stack_normf.values():\n",
    "        break\n",
    "\n",
    "    # process\n",
    "    processed = pred_fast_mt.predict_image_stack(stack_norm, margin=4, edge_size=30)\n",
    "    \n",
    "    # visualize\n",
    "    show_mdl_smpl(stack_norm, processed)\n",
    "    \n",
    "    print('saving dataset ', ds_idx, end='\\r')\n",
    "    path = datasets_path + '%d/'%ds_idx + '/pred_for_algn/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for i, img in enumerate(processed):\n",
    "        pimg = Image.fromarray(img[..., 0])\n",
    "        pimg.save(path+ 'img_%03d.png'%i, quality=99)\n",
    "        \n",
    "    \n",
    "    # make maskgen bat file and run if needed\n",
    "    make_genmask_bat(ds_idx, stck_tmpl, num)\n",
    "    \n",
    "    if auto_proc:\n",
    "        bat_file = gen_collective_genmask([ds_idx])\n",
    "        start_remote_job(datasets_path, bat_file)\n",
    "\n",
    "if not auto_proc:\n",
    "    for ds_idx, stck_tmpl, num in zip(datasets_ids, datasets_tmplts, nums):\n",
    "        make_genmask_bat(ds_idx, stck_tmpl, num)\n",
    "    bat_file = gen_collective_genmask(datasets_ids)\n",
    "    \n",
    "t1 = timer()\n",
    "print(t1 - t0, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Generate Histogram normalized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# wait masks done\n",
    "i = len(datasets_tmplts) - 1\n",
    "idx = datasets_ids[i]\n",
    "test_file = datasets_path +'%d/'%idx+'imgs_aligned_all/bin_mask_bg/%03d.png' % (nums[i]-1)\n",
    "\n",
    "while not os.path.exists(test_file):\n",
    "    time.sleep(5)\n",
    "    \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nh = NormHist('../data/data_prep/ref_imgs/%02d' % rds_id, dev=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, tmpl, tmpl_hn, boundaries, num in zip(datasets_ids, datasets_tmplts, datasets_hist_normed, block_boundaries, nums):\n",
    "    print(tmpl)\n",
    "    stack = iio.read_image_stack(tmpl, num, 1)\n",
    "    if len(stack.shape)==4:\n",
    "        stack = stack[...,0]\n",
    "        \n",
    "    stack_normed_blocks = []\n",
    "    \n",
    "    mask_tmpl = datasets_path +'%d/'%i+'imgs_aligned_all/bin_mask_bg/%03d.png'\n",
    "\n",
    "    \n",
    "    for idx, block_boundary in enumerate(boundaries):\n",
    "        bl_begin, bl_end = block_boundary\n",
    "        stack_src = stack[bl_begin: bl_end]\n",
    "        stack_src_ref = stack_src[-5:] if idx==0 else stack_src[:5]\n",
    "        print('normalizing block', idx, 'range', block_boundary, '...', end='\\r')\n",
    "        \n",
    "        mask_start_idx = (bl_end - 5) if idx==0 else bl_begin\n",
    "        mask_ref = iio.read_image_stack(mask_tmpl, 5, mask_start_idx)\n",
    "        if len(mask_ref.shape)==4:\n",
    "            mask_ref = mask_ref[...,0]\n",
    "            \n",
    "        stack_normed_block = nh.correct_stack(stack_src, stack_src_ref, mask_ref)\n",
    "        #fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        #ax[0].plot(nh.last_lut)\n",
    "        #ax[1].plot(nh.last_lut[200:])\n",
    "        #plt.show()\n",
    "        stack_normed_blocks.append(stack_normed_block)\n",
    "\n",
    "        \n",
    "    stack_normed_blocks = np.concatenate(stack_normed_blocks, axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    print('saving...                                     ', end='\\r')\n",
    "    os.makedirs(os.path.dirname(tmpl_hn), exist_ok=True)\n",
    "    for idx, im in enumerate(stack_normed_blocks):\n",
    "        iio.save_image(im, tmpl_hn % idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generate bat file for alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen alignment  bat file\n",
    "prog = proc_bin_path+'TimeFramesAligner_64.exe '\n",
    "cfg = f'-cfg:{proc_bin_path}TFAligner.cfg '\n",
    "\n",
    "\n",
    "rmdir_statement = lambda path: \"\"\"IF exist %s (\n",
    "  rmdir /s /q %s\n",
    ")\\n\"\"\" % (path, path)\n",
    "\n",
    "for ds_idx, stck_tmpl, stck_aux, has_aux, num  in zip(datasets_ids, datasets_hist_normed, datasetsf_tmplts, fluo_present, nums):\n",
    "    mask_tmpl = datasets_path +'%d/'%ds_idx+'pred_for_algn/'+'img_%03d.png'\n",
    "    tgt_dir = datasets_path +'%d/'%ds_idx+'imgs_aligned_all/'\n",
    "    s_t = proc_path(stck_tmpl)\n",
    "    m_t = proc_path(mask_tmpl)\n",
    "    tgd = proc_path(tgt_dir)\n",
    "    \n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd %~dp0\\n'\n",
    "    cmd += prog+cfg\n",
    "    cmd += '-savedir:\"%s\" ' % tgd\n",
    "    cmd += '-n_itr:3 '\n",
    "    cmd += '-stack:\"%s\" ' % s_t\n",
    "    cmd += '-raw_idx_0:0 '\n",
    "    cmd += '-mask:\"%s\" ' % m_t\n",
    "    cmd += '-n_frames:%d ' %  num\n",
    "    \n",
    "    if has_aux:\n",
    "        for aux_id, aux_tmpl in enumerate(stck_aux):\n",
    "            s_t = proc_path(aux_tmpl)\n",
    "            cmd += '-stack_%d:\"%s\" ' % (aux_id, s_t)\n",
    "            cmd += '-stack_%d_subpixel ' % aux_id\n",
    "            cmd += '-stack_%d_start:1 ' % aux_id\n",
    "        \n",
    "    cmd += '\\n'\n",
    "    \n",
    "    cmd += rmdir_statement('\"%sraw\"'%tgd)\n",
    "    cmd += 'move \"%scorrected\" \"%sraw\"\\n' % (tgd, tgd)\n",
    "\n",
    "    if has_aux:\n",
    "        for aux_id, _ in enumerate(stck_aux):\n",
    "            cmd += rmdir_statement('\"%sflr%d\"'%(tgd, aux_id+1))\n",
    "            cmd += 'move \"%scorrected_st_%d\" \"%sflr%d\"\\n' % (tgd, aux_id, tgd, aux_id+1)\n",
    "    cmd += 'popd\\n'\n",
    "        \n",
    "    bat_file = datasets_path + '%d/'%ds_idx + 'align.bat'\n",
    "    #print(cmd)\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen all alignments bat file\n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in datasets_ids:\n",
    "    cmd += 'call %d'%ds_idx+'\\\\align.bat \\n'\n",
    "    \n",
    "first_last = str(datasets_ids[0])\n",
    "if len(datasets_ids)>1:\n",
    "    first_last += '-'+str(datasets_ids[-1])\n",
    "bat_file = datasets_path + 'align_[' + first_last + '].bat'\n",
    "with open(bat_file, 'wt') as f:\n",
    "    f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if auto_proc:\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        sleep(5)\n",
    "    shutil.copy(bat_file, datasets_path + 'remote.bat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load aligned datasets, normalize and save normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait alignment done\n",
    "i = len(al_datasets_tmplts) - 1\n",
    "test_file = al_datasets_tmplts[i]%(nums[i]-1)  # last dataset, first file\n",
    "while not os.path.exists(test_file):\n",
    "    time.sleep(5)\n",
    "    \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# normalization\n",
    "for tmpl, nnrm, num in zip(al_datasets_tmplts, al_datasets_normed, nums):\n",
    "    print(tmpl)\n",
    "    stack = iio.read_image_stack(tmpl, num)\n",
    "    stack_center = stack[:, 130:-130, 130:-130]\n",
    "    stack_norm = pr.Predictor.normalize_stack(stack=stack, \n",
    "                                              norm_stack=stack_center, \n",
    "                                              normalization_percentile_range=(2.5, 97.5)\n",
    "                                             )\n",
    "    np.savez(nnrm, stack_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prediction by aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = timer()\n",
    "for i, (nnrm, ds_idx) in enumerate(zip(al_datasets_normed, datasets_ids)):\n",
    "    # load normalized\n",
    "    stack_normf = np.load(nnrm+'.npz')\n",
    "    for stack_norm in stack_normf.values():\n",
    "        break\n",
    "    # process\n",
    "    processed = pred_mt.predict_image_stack(stack_norm, margin=8, edge_size=60)\n",
    "    \n",
    "    #save processed\n",
    "    print(ds_idx, end='\\r')\n",
    "    pref = os.path.join(datasets_path,'%d'%ds_idx)\n",
    "    path_cell = os.path.join(pref,'pred_cdc', 'cell')\n",
    "    path_diap = os.path.join(pref,'pred_cdc', 'diap')\n",
    "    path_cntr = os.path.join(pref,'pred_cdc', 'cntr')\n",
    "    path_cntC = os.path.join(pref,'pred_cdc', 'cntC')\n",
    "    \n",
    "    os.makedirs(path_cell, exist_ok=True)\n",
    "    os.makedirs(path_diap, exist_ok=True)\n",
    "    os.makedirs(path_cntr, exist_ok=True)\n",
    "    os.makedirs(path_cntC, exist_ok=True)\n",
    "    \n",
    "    for i, img  in enumerate(processed):\n",
    "        for t, path in enumerate([path_cell, path_diap, path_cntr]):\n",
    "            pimg = Image.fromarray(img[..., t])\n",
    "            im_path = os.path.join(path, 'img_%03d.png'%i)\n",
    "            pimg.save(im_path , quality=99)\n",
    "            \n",
    "        cell_im = img[..., 0]\n",
    "        cntr_im = img[..., 2].copy()\n",
    "        mask_no_cell = cell_im <= 85\n",
    "        cntr_im[mask_no_cell] = 0\n",
    "\n",
    "        pimg = Image.fromarray(cntr_im)\n",
    "        im_path = os.path.join(path_cntC, 'img_%03d.png'%i)\n",
    "        pimg.save(im_path , quality=99)\n",
    "        \n",
    "    del processed\n",
    "    del stack_norm\n",
    "        \n",
    "        \n",
    "t1 = timer()\n",
    "print(t1 - t0, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tile merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_tiles_ids = list(tiles_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#gen lists\n",
    "for merged_idx, ti in tiles_info.items():\n",
    "    ny, nx = ti[0]\n",
    "    tile_ids = ti[1]\n",
    "    \n",
    "    info_path = os.path.join(datasets_path, '%d' % merged_idx, 'tiles_info')\n",
    "    os.makedirs(info_path, exist_ok=True)\n",
    "    \n",
    "    lists = ['', '', '', '', '', '']\n",
    "    \n",
    "    for idx in tile_ids:\n",
    "        lists[0] += datasets_path_proc + '%d' % idx + r'\\imgs_aligned_all\\raw\\%03d.png'+'\\n'\n",
    "        lists[1] += datasets_path_proc + '%d' % idx + r'\\pred_cdc\\cell\\img_%03d.png'+'\\n'\n",
    "        lists[2] += datasets_path_proc + '%d' % idx + r'\\pred_cdc\\diap\\img_%03d.png'+'\\n'\n",
    "        lists[3] += datasets_path_proc + '%d' % idx + r'\\pred_cdc\\cntC\\img_%03d.png'+'\\n'\n",
    "        lists[4] += datasets_path_proc + '%d' % idx + r'\\imgs_aligned_all\\flr1\\%03d.png'+'\\n'\n",
    "        lists[5] += datasets_path_proc + '%d' % idx + r'\\imgs_aligned_all\\flr2\\%03d.png'+'\\n'\n",
    "        \n",
    "    tmap = '%d %d\\n' % (nx, ny)\n",
    "    idx = -1\n",
    "    for iy in range(ny):\n",
    "        for ix in range(nx):\n",
    "            idx += 1\n",
    "            \n",
    "            tmap += '%d %d %d %d %d\\n' % (idx, ix, iy, ix * tile_dx, iy*tile_dy)\n",
    "            \n",
    "    for tlist, fname in zip(lists, ['raw.tl', 'cell.tl', 'diap.tl', 'cntc.tl', 'flr1.tl', 'flr2.tl']):\n",
    "        fpath = os.path.join(info_path, fname)\n",
    "        with open(fpath, 'wt') as f:\n",
    "            f.write(tlist)\n",
    "            \n",
    "    fpath = os.path.join(info_path, 'map.tm')\n",
    "    with open(fpath, 'wt') as f:\n",
    "        f.write(tmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen merging bat file\n",
    "prog_tal = proc_bin_path + 'proc_iv.bat'\n",
    "\n",
    "\n",
    "ds_id_to_idx = {i:idx for idx, i in enumerate(datasets_ids)}\n",
    "\n",
    "\n",
    "for idx in tiles_info:\n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd '+proc_bin_path+'\\n'\n",
    "    \n",
    "    tile_idx = ds_id_to_idx[tiles_info[idx][1][0]]\n",
    "    n_tf = nums[tile_idx]\n",
    "    cmd += 'call ' + prog_tal + ' %d' % idx  + ' %d' % n_tf + ' %d' % len(datasetsf_tmplts[tile_idx])\n",
    "    cmd += '\\n'\n",
    "    cmd += 'popd\\n'\n",
    "\n",
    "    bat_file = datasets_path + '%d/'%idx + 'merge.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen all merging bat file\n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in tiles_info:\n",
    "    cmd += 'call %d'%ds_idx+'\\\\merge.bat \\n'\n",
    "bat_file = datasets_path + 'merge_' + str(merged_tiles_ids) + '.bat'\n",
    "with open(bat_file, 'wt') as f:\n",
    "    f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if auto_proc:\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        sleep(5)\n",
    "    shutil.copy(bat_file, datasets_path + 'remote.bat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cell segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen segmentation bat file\n",
    "prog_seg = proc_bin_path + 'proc_ds_flr_n.bat'\n",
    "\n",
    "for i, idx in enumerate(datasets_ids+merged_tiles_ids):\n",
    "    test_idx = i if idx not in merged_tiles_ids else ds_id_to_idx[tiles_info[idx][1][0]]\n",
    "    has_flour = fluo_present[test_idx]\n",
    "    \n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd '+proc_bin_path+'\\n'\n",
    "    cmd += 'call ' + prog_seg + ' %d' % idx  + ' %d' % nums[test_idx]+ ' %d' % len(datasetsf_tmplts[test_idx])\n",
    "    cmd += '\\n'\n",
    "    cmd += 'popd\\n'\n",
    "\n",
    "    bat_file = datasets_path + '%d/'%idx + 'segment.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen all segmentation bat file\n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in datasets_ids:\n",
    "    cmd += 'call %d'%ds_idx+'\\\\segment.bat \\n'\n",
    "\n",
    "first_last = str(datasets_ids[0])\n",
    "if len(datasets_ids)>1:\n",
    "    first_last += '-'+str(datasets_ids[-1])\n",
    "\n",
    "bat_file_t = datasets_path + 'segment_[' + first_last + '].bat'\n",
    "with open(bat_file_t, 'wt') as f:\n",
    "    f.write(cmd)\n",
    "    \n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in merged_tiles_ids:\n",
    "    cmd += 'call %d'%ds_idx+'\\\\segment.bat \\n'\n",
    "\n",
    "first_last = str(merged_tiles_ids[0])\n",
    "if len(merged_tiles_ids)>1:\n",
    "    first_last += '-'+str(merged_tiles_ids[-1])\n",
    "bat_file_m = datasets_path + 'segment_[' + first_last + '].bat'\n",
    "with open(bat_file_m, 'wt') as f:\n",
    "    f.write(cmd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# process tiles\n",
    "if auto_proc:\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        time.sleep(5)\n",
    "    shutil.copy(bat_file_t, datasets_path + 'remote.bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait merging and segmnet merged\n",
    "if auto_proc:\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        time.sleep(5)\n",
    "    shutil.copy(bat_file_m, datasets_path + 'remote.bat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# wait segmentation done\n",
    "test_file = datasets_path + '%d/segmentation/cells/cells.gtp' % datasets_ids[-1]  # last dataset, cells file\n",
    "while not os.path.exists(test_file):\n",
    "    time.sleep(5)\n",
    "    \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait segmentation done\n",
    "test_file = datasets_path + '%d/segmentation/cells/cells.gtp' % merged_tiles_ids[-1]  # last dataset, cells file\n",
    "while not os.path.exists(test_file):\n",
    "    time.sleep(5)\n",
    "    \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# gen export bat file\n",
    "path_tra = f'call {proc_bin_path}proc_export.bat'\n",
    "\n",
    "for i, idx in enumerate(datasets_ids + merged_tiles_ids):\n",
    "    cmd = 'pushd %~dp0\\n'\n",
    "    cmd += path_tra + ' %d' % idx\n",
    "    cmd += '\\n'\n",
    "    cmd += 'popd\\n'\n",
    "\n",
    "    bat_file = datasets_path + '%d/'%idx + 'export.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_merged_tiles_ids = merged_tiles_ids # can have subrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# gen all export bat file\n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in datasets_ids:\n",
    "    cmd += 'call %d'%ds_idx+'\\\\export.bat \\n'\n",
    "    \n",
    "first_last = str(datasets_ids[0])\n",
    "if len(datasets_ids)>1:\n",
    "    first_last += '-'+str(datasets_ids[-1])\n",
    "\n",
    "bat_file_t = datasets_path + 'track_[' + first_last + '].bat'\n",
    "with open(bat_file_t, 'wt') as f:\n",
    "    f.write(cmd)\n",
    "    \n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in sel_merged_tiles_ids:\n",
    "    cmd += 'call %d'%ds_idx+'\\\\export.bat \\n'\n",
    "    \n",
    "    \n",
    "first_last = str(sel_merged_tiles_ids[0])\n",
    "if len(sel_merged_tiles_ids)>1:\n",
    "    first_last += '-'+str(sel_merged_tiles_ids[-1])\n",
    "bat_file_m = datasets_path + 'track_[' + first_last + '].bat'\n",
    "with open(bat_file_m, 'wt') as f:\n",
    "    f.write(cmd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "if auto_proc:\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        time.sleep(5)\n",
    "    shutil.copy(bat_file_t, datasets_path + 'remote.bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if auto_proc:\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        time.sleep(5)\n",
    "    shutil.copy(bat_file_m, datasets_path + 'remote.bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if auto_proc:\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_end_t = timer()\n",
    "print(f'notebook run time: {(nb_end_t - nb_start_t):.2f} sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
