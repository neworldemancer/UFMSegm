{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of dataset XXX by XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Copyright 2015-2023, University of Bern, Laboratory for High Energy Physics and Theodor Kocher Institute, M. Vladymyrov\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:30:10.471000Z",
     "start_time": "2023-09-11T11:30:10.463000Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "sys.path.append('../model training')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import interp1d as interp\n",
    "\n",
    "from utils import net_utils as nu\n",
    "from utils import imgio as iio\n",
    "from utils import predictor as pr\n",
    "from utils.histnorm import NormHist\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time as timer\n",
    "from time import sleep\n",
    "import shutil\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "from threading import Thread\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:30:11.057000Z",
     "start_time": "2023-09-11T11:30:11.055000Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_start_t = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:30:11.730000Z",
     "start_time": "2023-09-11T11:30:11.727000Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets_ids = list(range(0, 8)) # range of tiled datasets, expected to be last folders, and 8 tiles\n",
    "merged_ds_idx0 = datasets_ids[-1] +1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and trained models location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:21.970000Z",
     "start_time": "2023-09-11T11:31:21.966000Z"
    }
   },
   "outputs": [],
   "source": [
    "# path from inference node\n",
    "datasets_path = os.path.abspath('../../datasets_seg')+'/'\n",
    "\n",
    "models_path = os.path.abspath('../../trained models')+'/'\n",
    "ref_ds_path = models_path+'ref datasets/'\n",
    "\n",
    "# path from win processing node. Should be consistent with cfg files.\n",
    "# Remote processing should be avoided for performance reasons,\n",
    "# but if used, should be within closed private network\n",
    "\n",
    "datasets_path_proc = 'Q:\\\\' #'D:\\\\UFMTrack\\\\datasets_seg\\\\' # \n",
    "proc_bin_path = 'C:\\\\VivoFollow\\\\bin\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:23.457000Z",
     "start_time": "2023-09-11T11:31:23.448000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_ids = [0]  # [0, 1, 2, 3, 4, 5, 6, 7] # list of available GPUs\n",
    "dev_id = dev_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:23.994000Z",
     "start_time": "2023-09-11T11:31:23.991000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "auto_proc = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fill datasets info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:25.404000Z",
     "start_time": "2023-09-11T11:31:25.353000Z"
    },
    "code_folding": [
     19
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rds_id = 7 # ref. ds. id\n",
    "\n",
    "nums=[]\n",
    "datasets_names = []\n",
    "fluo_present = []\n",
    "\n",
    "datasets_tmplts = []\n",
    "datasetsf_tmplts = []\n",
    "datasets_normed = []\n",
    "datasets_hist_normed = []\n",
    "\n",
    "block_boundaries = []\n",
    "\n",
    "datasets_is_tile = []\n",
    "datasets_tile_idx = []\n",
    "datasets_tile_group_id = []\n",
    "datasets_hw = []  # image hight/width\n",
    "\n",
    "tile_sep = '_tile'\n",
    "for ds_idx in datasets_ids:\n",
    "    # read info file\n",
    "    info_file_name = datasets_path + '%03d/info.txt' % ds_idx\n",
    "    with open(info_file_name, 'rt') as f:\n",
    "        ds_name = f.readline()\n",
    "    \n",
    "    #list files in dir\n",
    "    ds_path = datasets_path + '%03d/%s/' % (ds_idx, ds_name)\n",
    "    all_tif_files = [n for n in os.listdir(ds_path) if ds_name in n]\n",
    "    \n",
    "    all_sfx = sorted([n.replace(ds_name+'_t', '').replace('.tif', '') for n in all_tif_files])\n",
    "    last_sfx = all_sfx[-1]\n",
    "    \n",
    "    is_tile = tile_sep in ds_name\n",
    "    if is_tile:\n",
    "        grpid_idx = ds_name.split(tile_sep)\n",
    "        assert len(grpid_idx)==2, f'unexpected dataset name format: \"{ds_name}\", contains multiple \"{tile_sep}\"'\n",
    "        tile_group_id, tile_idx = grpid_idx\n",
    "        tile_idx = int(tile_idx) - 1\n",
    "    else:\n",
    "        tile_group_id, tile_idx = '', -1\n",
    "    \n",
    "    #get # channel, #time points\n",
    "    has_fluo = 'c' in last_sfx\n",
    "    \n",
    "    if has_fluo:\n",
    "        n_t_c_s = last_sfx.split('c')\n",
    "        n_t, n_c = [int(s) for s in n_t_c_s]\n",
    "    else:\n",
    "        n_c = 1\n",
    "        n_t = int(last_sfx)\n",
    "    \n",
    "    #fill tmplts, num, fluo present, \n",
    "    tmpl_t = '_t%0' + '%d' % len('%d' % n_t) + 'd'\n",
    "    tmpl_c = 'c%0' + '%d' % len('%d' % n_c if has_fluo else '1') + 'd'\n",
    "    \n",
    "    dataset_tmplt = ds_path + ds_name + tmpl_t + (tmpl_c % 1 if has_fluo else '') +'.tif'\n",
    "    datasetf_tmplts = [ds_path + ds_name + tmpl_t + (tmpl_c % ch) +'.tif' for ch in range(2, n_c+1)] if has_fluo else []\n",
    "    dataset_normed = ds_path + 'normed'\n",
    "    \n",
    "    dataset_hist_normed =  datasets_path + '%03d/' % ds_idx + 'hist_normed/img_%03d.tif' #png\n",
    "\n",
    "    #print(n_t, n_c, has_fluo, tmpl_t, tmpl_c, dataset_tmplt, datasetf_tmplts, dataset_normed)\n",
    "    \n",
    "    stack = iio.read_image_stack(dataset_tmplt, 1, 1)\n",
    "    hw = stack.shape[1:]\n",
    "    \n",
    "    # use num per dataset\n",
    "    nums.append(n_t)\n",
    "    datasets_names.append(ds_name)\n",
    "    \n",
    "    datasets_is_tile.append(is_tile)\n",
    "    datasets_tile_idx.append(tile_idx)\n",
    "    datasets_tile_group_id.append(tile_group_id)\n",
    "    \n",
    "    datasets_hw.append(hw)\n",
    "    \n",
    "    fluo_present.append(has_fluo)\n",
    "\n",
    "    datasets_tmplts.append(dataset_tmplt)\n",
    "    datasetsf_tmplts.append(datasetf_tmplts)\n",
    "    datasets_normed.append(dataset_normed)\n",
    "    \n",
    "    datasets_hist_normed.append(dataset_hist_normed)\n",
    "    \n",
    "\n",
    "    block_info_path = os.path.join(datasets_path, '%03d' % ds_idx, 'block_info.txt')\n",
    "    if os.path.exists(block_info_path):\n",
    "        with open(block_info_path, 'rt') as f:\n",
    "            txt = f.readline()\n",
    "    else:\n",
    "        txt = ''\n",
    "    if txt:\n",
    "        block_boundary = [[int(bi) for bi in b.split(' ')] for b in txt.split('|')]\n",
    "    else:\n",
    "        block_boundary = [[0, n_t]]\n",
    "    block_boundaries.append(block_boundary)\n",
    "    \n",
    "al_datasets_tmplts = [datasets_path +'%03d' % i+ '/imgs_aligned_all/raw/%03d.png' for i,n in zip(datasets_ids,datasets_names)]\n",
    "al_datasets_normed = [datasets_path +'%03d' % i+ '/imgs_aligned_all/normed' for i,n in zip(datasets_ids,datasets_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:26.899000Z",
     "start_time": "2023-09-11T11:31:26.881000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_tiled_groups(datasets_is_tile, \n",
    "                     datasets_tile_group_id, datasets_tile_idx,\n",
    "                     datasets_ids, datasets_hw, merged_ds_start_idx,\n",
    "                    ):\n",
    "    all_tile_dataset_ids = []\n",
    "    all_non_tile_dataset_ids = []\n",
    "    \n",
    "    tiles_info = {} #id: [[ny, nx, tile_dx, tile_dy], list_of_tile_dataset_ids],\n",
    "    \n",
    "    curr_group_id = None\n",
    "    curr_group_ds_idxs = []\n",
    "    \n",
    "    def get_group_size_ofs(group_hw, n_tiles):\n",
    "        group_hw = np.array(group_hw)\n",
    "        # print(group_hw)\n",
    "        mean_hw = group_hw.mean(axis=0)\n",
    "        \n",
    "        deviation_hw = np.abs(mean_hw-group_hw) / mean_hw\n",
    "        incompatible_hw = deviation_hw>0.01 # 1% difference is too much\n",
    "        \n",
    "        assert not np.any(incompatible_hw), 'size of time tiles are too different'\n",
    "        # print(group_hw, mean_hw, incompatible_hw)\n",
    "        \n",
    "        h, w = mean_hw\n",
    "        overlap = w * 0.09  # 10% of width\n",
    "        tile_dx = int(w - overlap)\n",
    "        tile_dy = int(h - overlap)\n",
    "        \n",
    "        # simple rules for identifying tile configuration:\n",
    "        ny_nx = []\n",
    "        ny_p_nx = []\n",
    "        for ny in range(1, 1+int(np.floor(np.sqrt(n_tiles)))):\n",
    "            nx = n_tiles // ny\n",
    "            res = n_tiles - ny * nx\n",
    "            if res == 0:\n",
    "                ny_nx.append([ny, nx])\n",
    "                ny_p_nx.append(ny+nx)\n",
    "        idx = np.argmin(ny_p_nx)\n",
    "        ny, nx = ny_nx[idx]\n",
    "        \n",
    "        #print(ny_nx)\n",
    "        #print(ny_nx[idx])\n",
    "\n",
    "        return [ny, nx, tile_dx, tile_dy]\n",
    "    \n",
    "    def fill_group(curr_group_ds_idxs, datasets_ids, datasets_hw,\n",
    "                   all_tile_dataset_ids, all_non_tile_dataset_ids,\n",
    "                   tiles_info, group_idx):\n",
    "\n",
    "        if len(curr_group_ds_idxs) < 2:\n",
    "            # cancel group\n",
    "            all_non_tile_dataset_ids.extend([datasets_ids[idx] for idx in curr_group_ds_idxs])\n",
    "        else:\n",
    "            groups_ds_ids = [datasets_ids[idx] for idx in curr_group_ds_idxs]\n",
    "            print(f'processing group {group_idx}, ds ids: {groups_ds_ids}')\n",
    "            all_tile_dataset_ids.extend(groups_ds_ids)\n",
    "\n",
    "            n_tiles = len(groups_ds_ids)\n",
    "            \n",
    "            group_hw = [datasets_hw[idx] for idx in curr_group_ds_idxs]\n",
    "            \n",
    "            size_ofs = get_group_size_ofs(group_hw, n_tiles)\n",
    "            tiles_info[merged_ds_start_idx + group_idx[0]] = [size_ofs, groups_ds_ids]\n",
    "\n",
    "            group_idx[0] += 1\n",
    "    \n",
    "    \n",
    "    group_idx = [0]  # in a list to be modified inside fill_group function\n",
    "    for idx, (is_tile, group_id, tile_idx, ds_id) in enumerate(zip(datasets_is_tile, \n",
    "                                                                   datasets_tile_group_id, \n",
    "                                                                   datasets_tile_idx, \n",
    "                                                                   datasets_ids)):\n",
    "        group_close = (curr_group_id is not None) and (not is_tile or group_id != curr_group_id)\n",
    "        \n",
    "        if group_close:\n",
    "            fill_group(curr_group_ds_idxs, datasets_ids, datasets_hw,\n",
    "                       all_tile_dataset_ids, all_non_tile_dataset_ids,\n",
    "                       tiles_info, group_idx)\n",
    "            \n",
    "            curr_group_id = None\n",
    "            curr_group_ds_idxs = []\n",
    "                \n",
    "        if not is_tile:\n",
    "            all_non_tile_dataset_ids.append(ds_id)\n",
    "        else:\n",
    "            assert (curr_group_id==group_id or curr_group_id is None)\n",
    "            curr_group_id = group_id\n",
    "            assert tile_idx==len(curr_group_ds_idxs)\n",
    "            curr_group_ds_idxs.append(idx)\n",
    "            \n",
    "    \n",
    "    if curr_group_id is not None:\n",
    "        fill_group(curr_group_ds_idxs, datasets_ids, datasets_hw, \n",
    "                       all_tile_dataset_ids, all_non_tile_dataset_ids,\n",
    "                       tiles_info, group_idx)\n",
    "        \n",
    "    return all_non_tile_dataset_ids, all_tile_dataset_ids, tiles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:27.359000Z",
     "start_time": "2023-09-11T11:31:27.356000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_non_tile_dataset_ids, all_tile_dataset_ids, tiles_info = get_tiled_groups(datasets_is_tile, \n",
    "                                                                              datasets_tile_group_id, datasets_tile_idx,\n",
    "                                                                              datasets_ids, datasets_hw,\n",
    "                                                                              merged_ds_start_idx=merged_ds_idx0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## MT predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:31.001000Z",
     "start_time": "2023-09-11T11:31:30.988000Z"
    },
    "code_folding": [
     0,
     1,
     20,
     23,
     33
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PredictorMT:\n",
    "    def __init__(self, mod_path, mod_itr,\n",
    "                 gpu_ids,\n",
    "                 batch_sz, input_sz,\n",
    "                 io_map,\n",
    "                 z_border=2  # one side\n",
    "                ):\n",
    "        self.z_border = z_border\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.n_thr = len(self.gpu_ids)\n",
    "        \n",
    "        self.predictors = [pr.Predictor(mod_path, mod_itr,\n",
    "                                        device_id=None,\n",
    "                                        device_ids=gpu_ids,\n",
    "                                        gpuid=i,\n",
    "                                        batch_sz=batch_sz, input_sz=input_sz,\n",
    "                                        in_out_dict=io_map,\n",
    "                                       ) for i, dev_id in enumerate(gpu_ids)]\n",
    "        self.res = []\n",
    "        \n",
    "    def normalize_stack(self, stack, norm_stack, normalization_percentile_range):\n",
    "        return self.predictors[0].normalize_stack(stack, norm_stack, normalization_percentile_range)\n",
    "    \n",
    "    def split_stack(self, stack):\n",
    "        n_z = len(stack)\n",
    "        n_z_chunk = (n_z + self.n_thr - 1) // self.n_thr\n",
    "        \n",
    "        begin_end_pairs = [[max(0, i*n_z_chunk-self.z_border), \n",
    "                            min(n_z, (i+1)*n_z_chunk+self.z_border)] for i in range(self.n_thr)]\n",
    "        \n",
    "        chunks = [stack[b:e] for b,e in begin_end_pairs]\n",
    "        return chunks\n",
    "    \n",
    "    def merge_result(self, stacks):\n",
    "        #self.z_border\n",
    "        \n",
    "        cropped_overlap = [\n",
    "                            s[0 if i==0 else self.z_border:\n",
    "                              len(s) if i==(self.n_thr-1) else -self.z_border\n",
    "                             ]\n",
    "                            for i, s in enumerate(stacks)\n",
    "                          ]\n",
    "        \n",
    "        res = np.concatenate(cropped_overlap, axis=0)\n",
    "        return res\n",
    "    \n",
    "    def _predict_image_stack(self, stack_norm, margin, keep_edge, edge_size, thr_idx):\n",
    "        pred = self.predictors[thr_idx]\n",
    "        res = pred.predict_image_stack(stack_norm, margin, keep_edge, edge_size)\n",
    "        self.res[thr_idx] = res\n",
    "        \n",
    "    def predict_image_stack(self, stack_norm, margin, keep_edge=True, edge_size=0):\n",
    "        # split stack\n",
    "        chunks = self.split_stack(stack_norm)\n",
    "        \n",
    "        self.res = [None for i in range(self.n_thr)]\n",
    "        \n",
    "        #make threads\n",
    "        threads = [Thread(target = self._predict_image_stack,\n",
    "                          args = (chunks[i], margin, keep_edge, edge_size, i)\n",
    "                         ) for i in range(self.n_thr)]\n",
    "        \n",
    "        #run all\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        \n",
    "        #wait all\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        merged = self.merge_result(self.res)\n",
    "        #merge ouput\n",
    "        #processed.shape == (192, 1077, 1405, 3), i.e. plain np tensor\n",
    "        \n",
    "        return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Save constatnt models for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:33.652000Z",
     "start_time": "2023-09-11T11:31:31.799000Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load models\n",
    "# fast model\n",
    "mod_path = models_path+'/model_BBB_BN_TV_FCN4_HN_CDC_2D_2021.04.17_22-54'  # 2D, 1 tf 2021 best \n",
    "mod_itr = 30001\n",
    "\n",
    "io_map = {'in':'stack:0', 'out':'ModelOutput:0', 'out_channels':[0,1,2]}\n",
    "batch_sz=8\n",
    "\n",
    "print('AFTER EXECUTING THIS FIRST TIME FOR A MODEL (Needs single-threaded version!) - RESTART THE KERNEL')\n",
    "print(mod_path, mod_itr)\n",
    "# pred_fast = pr.Predictor(mod_path, mod_itr,\n",
    "#                          device_id=dev_id,\n",
    "#                          batch_sz=batch_sz, input_sz=[512,512],\n",
    "#                          in_out_dict=io_map)\n",
    "\n",
    "pred_fast_mt = PredictorMT(mod_path, mod_itr,\n",
    "                   gpu_ids=dev_ids,\n",
    "                   batch_sz=batch_sz, input_sz=[512,512],\n",
    "                   io_map=io_map)\n",
    "\n",
    "\n",
    "\n",
    "mod_path = models_path+'/model_BBB_BN_TV_FCNx16_HN_CDC_2021.07.06_16-25'  # 3D, 2021 best\n",
    "mod_itr = 35001\n",
    "\n",
    "io_map = {'in':'stack:0', 'out':'ModelOutput:0', 'out_channels':[0,1,2]}\n",
    "\n",
    "batch_sz=1\n",
    "\n",
    "print(mod_path, mod_itr)\n",
    "# pred = pr.Predictor(mod_path, mod_itr,\n",
    "#                     device_id=dev_id,\n",
    "#                     batch_sz=batch_sz, input_sz=[512,512],\n",
    "#                     in_out_dict=io_map)\n",
    "pred_mt = PredictorMT(mod_path, mod_itr,\n",
    "                   gpu_ids=dev_ids,\n",
    "                   batch_sz=batch_sz, input_sz=[512,512],\n",
    "                   io_map=io_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load datasets, save match size, normalize and save normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:31:51.519000Z",
     "start_time": "2023-09-11T11:31:38.891000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for tmpl, nnrm, num in zip(datasets_tmplts, datasets_normed, nums):\n",
    "    print(tmpl)\n",
    "    stack = iio.read_image_stack(tmpl, num, 1)\n",
    "    \n",
    "    print('saving...')\n",
    "    if len(stack.shape)==4:\n",
    "        stack = stack[...,0]\n",
    "    \n",
    "    for idx, im in enumerate(stack):\n",
    "        iio.save_image(im, tmpl % (idx+1))\n",
    "    \n",
    "    stack_center = stack[:, 130:-130, 130:-130]\n",
    "    \n",
    "    print('normalizing...')\n",
    "    stack_norm = pr.Predictor.normalize_stack(stack=stack, \n",
    "                                              norm_stack=stack_center, \n",
    "                                              normalization_percentile_range=(2.5, 97.5))\n",
    "    print('saving normalized...')\n",
    "    np.savez(nnrm, stack_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generate cell probability maps for alignment and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:29:31.608000Z",
     "start_time": "2023-08-24T11:29:31.594000Z"
    },
    "code_folding": [
     0,
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def proc_path(path):\n",
    "    return path.replace(datasets_path, datasets_path_proc).replace('/','\\\\').replace('%','%%')\n",
    "\n",
    "def make_genmask_bat(ds_idx, stck_tmpl, num):\n",
    "    # gen background mask generation bat file. Alignment pipeline is used\n",
    "    prog = proc_bin_path+'TimeFramesAligner_64.exe '\n",
    "    cfg = f'-cfg:{proc_bin_path}TFAligner_bg_mask.cfg '\n",
    "    \n",
    "\n",
    "    mask_tmpl = datasets_path +'%03d/'%ds_idx+'pred_for_algn/'+'img_%03d.png'\n",
    "    tgt_dir = datasets_path +'%03d/'%ds_idx+'imgs_aligned_all/'\n",
    "    s_t = proc_path(stck_tmpl)\n",
    "    m_t = proc_path(mask_tmpl)\n",
    "    tgd = proc_path(tgt_dir)\n",
    "    \n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd %~dp0\\n'\n",
    "    cmd += prog+cfg\n",
    "    cmd += '-savedir:\"%s\" ' % tgd\n",
    "    cmd += '-n_itr:0 '\n",
    "    cmd += '-stack:\"%s\" ' % s_t\n",
    "    cmd += '-mask:\"%s\" ' % m_t\n",
    "    cmd += '-n_frames:%d ' %  num\n",
    "    \n",
    "    cmd += '\\n'\n",
    "    \n",
    "    cmd += 'popd\\n'\n",
    "        \n",
    "    bat_file = datasets_path + '%03d/'%ds_idx + 'genmask.bat'\n",
    "    #print(cmd)\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:29:31.844000Z",
     "start_time": "2023-08-24T11:29:31.837000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gen_collective_genmask(ds_idx_list):\n",
    "    \"\"\"\n",
    "    Generated all genmask bat file: executes individual genmask\n",
    "    returns: full path to created batfile\n",
    "    \"\"\"\n",
    "    #cmd = '@echo off\\n'\n",
    "    cmd = ''\n",
    "\n",
    "    for ds_idx in ds_idx_list:\n",
    "        cmd += 'call %03d'%ds_idx+'\\\\genmask.bat \\n'\n",
    "    bat_file = datasets_path + 'genmask_' + str(ds_idx_list) + '.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)\n",
    "        \n",
    "    return bat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:29:32.649000Z",
     "start_time": "2023-08-24T11:29:32.644000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def start_remote_job(datasets_path, bat_file, iteration_sleep_time=5):\n",
    "    if bat_file is None:\n",
    "        return\n",
    "    \n",
    "    itr = 0\n",
    "    while(os.path.exists(datasets_path + 'remote.bat')):\n",
    "        if itr==0:\n",
    "            print('Waiting previous remote job to be done...')\n",
    "        itr += 1\n",
    "        sleep(iteration_sleep_time)\n",
    "    shutil.copy(bat_file, datasets_path + 'remote.bat')\n",
    "    \n",
    "def wait_for_file(file_path, iteration_sleep_time=5, end_sleep_time=10):\n",
    "    while not os.path.exists(file_path):\n",
    "        time.sleep(iteration_sleep_time)\n",
    "\n",
    "    time.sleep(end_sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:29:34.007000Z",
     "start_time": "2023-08-24T11:29:34.002000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_mdl_smpl(raw, cdc):\n",
    "    mdl=len(cdc)//2\n",
    "    _=iio.draw_samples(\n",
    "        (\n",
    "            raw[mdl,...,0],\n",
    "            cdc[mdl,...,0],\n",
    "            cdc[mdl,...,1],\n",
    "            cdc[mdl,...,2]\n",
    "        ), color_range=(0,256)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = timer()\n",
    "for i, (nnrm, num, ds_idx, stck_tmpl) in enumerate(zip(datasets_normed, nums, datasets_ids, datasets_tmplts)):\n",
    "    # read normalized\n",
    "    stack_normf = np.load(nnrm+'.npz')\n",
    "    for stack_norm in stack_normf.values():\n",
    "        break\n",
    "\n",
    "    # process\n",
    "    processed = pred_fast_mt.predict_image_stack(stack_norm, margin=4, edge_size=30)\n",
    "    \n",
    "    # visualize\n",
    "    show_mdl_smpl(stack_norm, processed)\n",
    "    \n",
    "    print('saving dataset ', ds_idx, end='\\r')\n",
    "    path = datasets_path + '%03d/'%ds_idx + '/pred_for_algn/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for i, img in enumerate(processed):\n",
    "        pimg = Image.fromarray(img[..., 0])\n",
    "        pimg.save(path+ 'img_%03d.png'%i, quality=99)\n",
    "        \n",
    "    # make maskgen bat file and run if needed\n",
    "    make_genmask_bat(ds_idx, stck_tmpl, num)\n",
    "    \n",
    "    if auto_proc:\n",
    "        bat_file = gen_collective_genmask([ds_idx])\n",
    "        start_remote_job(datasets_path, bat_file)\n",
    "\n",
    "if not auto_proc:\n",
    "    for ds_idx, stck_tmpl, num in zip(datasets_ids, datasets_tmplts, nums):\n",
    "        make_genmask_bat(ds_idx, stck_tmpl, num)\n",
    "    bat_file = gen_collective_genmask(datasets_ids)\n",
    "    \n",
    "t1 = timer()\n",
    "print(t1 - t0, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generate Histogram normalized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# wait masks done\n",
    "i = len(datasets_tmplts) - 1\n",
    "idx = datasets_ids[i]\n",
    "test_file = datasets_path +'%03d/'%idx+'imgs_aligned_all/bin_mask_bg/%03d.png' % (nums[i]-1)\n",
    "\n",
    "wait_for_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:29:59.600000Z",
     "start_time": "2023-08-24T11:29:45.861000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nh = NormHist(ref_ds_path+r'/%02d' % rds_id, dev=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, tmpl, tmpl_hn, boundaries, num in zip(datasets_ids, datasets_tmplts, datasets_hist_normed, block_boundaries, nums):\n",
    "    print(tmpl)\n",
    "    stack = iio.read_image_stack(tmpl, num, 1)\n",
    "    if len(stack.shape)==4:\n",
    "        stack = stack[...,0]\n",
    "        \n",
    "    stack_normed_blocks = []\n",
    "    \n",
    "    mask_tmpl = datasets_path +'%03d/'%i+'imgs_aligned_all/bin_mask_bg/%03d.png'\n",
    "\n",
    "    \n",
    "    for idx, block_boundary in enumerate(boundaries):\n",
    "        bl_begin, bl_end = block_boundary\n",
    "        stack_src = stack[bl_begin: bl_end]\n",
    "        stack_src_ref = stack_src[-5:] if idx==0 else stack_src[:5]\n",
    "        print('normalizing block', idx, 'range', block_boundary, '...', end='\\r')\n",
    "        \n",
    "        mask_start_idx = (bl_end - 5) if idx==0 else bl_begin\n",
    "        mask_ref = iio.read_image_stack(mask_tmpl, 5, mask_start_idx)\n",
    "        if len(mask_ref.shape)==4:\n",
    "            mask_ref = mask_ref[...,0]\n",
    "            \n",
    "        stack_normed_block = nh.correct_stack(stack_src, stack_src_ref, mask_ref)\n",
    "        #fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        #ax[0].plot(nh.last_lut)\n",
    "        #ax[1].plot(nh.last_lut[200:])\n",
    "        #plt.show()\n",
    "        stack_normed_blocks.append(stack_normed_block)\n",
    "\n",
    "        \n",
    "    stack_normed_blocks = np.concatenate(stack_normed_blocks, axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    print('saving...                                     ', end='\\r')\n",
    "    os.makedirs(os.path.dirname(tmpl_hn), exist_ok=True)\n",
    "    for idx, im in enumerate(stack_normed_blocks):\n",
    "        iio.save_image(im, tmpl_hn % idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generate bat file for alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen alignment  bat file\n",
    "prog = proc_bin_path+'TimeFramesAligner_64.exe '\n",
    "cfg = f'-cfg:{proc_bin_path}TFAligner.cfg '\n",
    "\n",
    "\n",
    "rmdir_statement = lambda path: \"\"\"IF exist %s (\n",
    "  rmdir /s /q %s\n",
    ")\\n\"\"\" % (path, path)\n",
    "\n",
    "for ds_idx, stck_tmpl, stck_aux, has_aux, num  in zip(datasets_ids, datasets_hist_normed, datasetsf_tmplts, fluo_present, nums):\n",
    "    mask_tmpl = datasets_path +'%03d/'%ds_idx+'pred_for_algn/'+'img_%03d.png'\n",
    "    tgt_dir = datasets_path +'%03d/'%ds_idx+'imgs_aligned_all/'\n",
    "    s_t = proc_path(stck_tmpl)\n",
    "    m_t = proc_path(mask_tmpl)\n",
    "    tgd = proc_path(tgt_dir)\n",
    "    \n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd %~dp0\\n'\n",
    "    cmd += prog+cfg\n",
    "    cmd += '-savedir:\"%s\" ' % tgd\n",
    "    cmd += '-n_itr:3 '\n",
    "    cmd += '-stack:\"%s\" ' % s_t\n",
    "    cmd += '-raw_idx_0:0 '\n",
    "    cmd += '-mask:\"%s\" ' % m_t\n",
    "    cmd += '-n_frames:%d ' %  num\n",
    "    \n",
    "    if has_aux:\n",
    "        for aux_id, aux_tmpl in enumerate(stck_aux):\n",
    "            s_t = proc_path(aux_tmpl)\n",
    "            cmd += '-stack_%d:\"%s\" ' % (aux_id, s_t)\n",
    "            cmd += '-stack_%d_subpixel ' % aux_id\n",
    "            cmd += '-stack_%d_start:1 ' % aux_id\n",
    "        \n",
    "    cmd += '\\n'\n",
    "    \n",
    "    cmd += rmdir_statement('\"%sraw\"'%tgd)\n",
    "    cmd += 'move \"%scorrected\" \"%sraw\"\\n' % (tgd, tgd)\n",
    "\n",
    "    if has_aux:\n",
    "        for aux_id, _ in enumerate(stck_aux):\n",
    "            cmd += rmdir_statement('\"%sflr%d\"'%(tgd, aux_id+1))\n",
    "            cmd += 'move \"%scorrected_st_%d\" \"%sflr%d\"\\n' % (tgd, aux_id, tgd, aux_id+1)\n",
    "    cmd += 'popd\\n'\n",
    "        \n",
    "    bat_file = datasets_path + '%03d/'%ds_idx + 'align.bat'\n",
    "    #print(cmd)\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen all alignments bat file\n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in datasets_ids:\n",
    "    cmd += 'call %03d'%ds_idx+'\\\\align.bat \\n'\n",
    "    \n",
    "first_last = str(datasets_ids[0])\n",
    "if len(datasets_ids)>1:\n",
    "    first_last += '-'+str(datasets_ids[-1])\n",
    "bat_file = datasets_path + 'align_[' + first_last + '].bat'\n",
    "with open(bat_file, 'wt') as f:\n",
    "    f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_remote_job(datasets_path, bat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load aligned datasets, normalize and save normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait alignment done\n",
    "i = len(al_datasets_tmplts) - 1\n",
    "test_file = al_datasets_tmplts[i]%(nums[i]-1)  # last dataset, first file\n",
    "while not os.path.exists(test_file):\n",
    "    time.sleep(5)\n",
    "    \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# normalization\n",
    "for tmpl, nnrm, num in zip(al_datasets_tmplts, al_datasets_normed, nums):\n",
    "    print(tmpl)\n",
    "    stack = iio.read_image_stack(tmpl, num)\n",
    "    stack_center = stack[:, 130:-130, 130:-130]\n",
    "    stack_norm = pr.Predictor.normalize_stack(stack=stack, \n",
    "                                              norm_stack=stack_center, \n",
    "                                              normalization_percentile_range=(2.5, 97.5)\n",
    "                                             )\n",
    "    np.savez(nnrm, stack_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prediction by aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = timer()\n",
    "for i, (nnrm, ds_idx) in enumerate(zip(al_datasets_normed, datasets_ids)):\n",
    "    # load normalized\n",
    "    stack_normf = np.load(nnrm+'.npz')\n",
    "    for stack_norm in stack_normf.values():\n",
    "        break\n",
    "    # process\n",
    "    processed = pred_mt.predict_image_stack(stack_norm, margin=8, edge_size=60)\n",
    "    \n",
    "    #save processed\n",
    "    print(ds_idx, end='\\r')\n",
    "    pref = os.path.join(datasets_path,'%03d'%ds_idx)\n",
    "    path_cell = os.path.join(pref,'pred_cdc', 'cell')\n",
    "    path_diap = os.path.join(pref,'pred_cdc', 'diap')\n",
    "    path_cntr = os.path.join(pref,'pred_cdc', 'cntr')\n",
    "    path_cntC = os.path.join(pref,'pred_cdc', 'cntC')\n",
    "    \n",
    "    os.makedirs(path_cell, exist_ok=True)\n",
    "    os.makedirs(path_diap, exist_ok=True)\n",
    "    os.makedirs(path_cntr, exist_ok=True)\n",
    "    os.makedirs(path_cntC, exist_ok=True)\n",
    "    \n",
    "    for i, img  in enumerate(processed):\n",
    "        for t, path in enumerate([path_cell, path_diap, path_cntr]):\n",
    "            pimg = Image.fromarray(img[..., t])\n",
    "            im_path = os.path.join(path, 'img_%03d.png'%i)\n",
    "            pimg.save(im_path , quality=99)\n",
    "            \n",
    "        cell_im = img[..., 0]\n",
    "        cntr_im = img[..., 2].copy()\n",
    "        mask_no_cell = cell_im <= 85\n",
    "        cntr_im[mask_no_cell] = 0\n",
    "\n",
    "        pimg = Image.fromarray(cntr_im)\n",
    "        im_path = os.path.join(path_cntC, 'img_%03d.png'%i)\n",
    "        pimg.save(im_path , quality=99)\n",
    "        \n",
    "    del processed\n",
    "    del stack_norm\n",
    "        \n",
    "        \n",
    "t1 = timer()\n",
    "print(t1 - t0, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tile merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:07.819000Z",
     "start_time": "2023-08-24T11:30:07.814000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_tiles_ids = list(tiles_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:09.015000Z",
     "start_time": "2023-08-24T11:30:08.988000Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#gen lists\n",
    "for merged_idx, ti in tiles_info.items():\n",
    "    ny, nx, tile_dx, tile_dy = ti[0]\n",
    "    tile_ids = ti[1]\n",
    "    \n",
    "    info_path = os.path.join(datasets_path, '%03d' % merged_idx, 'tiles_info')\n",
    "    os.makedirs(info_path, exist_ok=True)\n",
    "    \n",
    "    lists = ['', '', '', '', '', '']\n",
    "    \n",
    "    for idx in tile_ids:\n",
    "        lists[0] += datasets_path_proc + '%03d' % idx + r'\\imgs_aligned_all\\raw\\%03d.png'+'\\n'\n",
    "        lists[1] += datasets_path_proc + '%03d' % idx + r'\\pred_cdc\\cell\\img_%03d.png'+'\\n'\n",
    "        lists[2] += datasets_path_proc + '%03d' % idx + r'\\pred_cdc\\diap\\img_%03d.png'+'\\n'\n",
    "        lists[3] += datasets_path_proc + '%03d' % idx + r'\\pred_cdc\\cntC\\img_%03d.png'+'\\n'\n",
    "        lists[4] += datasets_path_proc + '%03d' % idx + r'\\imgs_aligned_all\\flr1\\%03d.png'+'\\n'\n",
    "        lists[5] += datasets_path_proc + '%03d' % idx + r'\\imgs_aligned_all\\flr2\\%03d.png'+'\\n'\n",
    "        \n",
    "    tmap = '%d %d\\n' % (nx, ny)\n",
    "    idx = -1\n",
    "    for iy in range(ny):\n",
    "        for ix in range(nx):\n",
    "            idx += 1\n",
    "            \n",
    "            tmap += '%d %d %d %d %d\\n' % (idx, ix, iy, ix * tile_dx, iy*tile_dy)\n",
    "            \n",
    "    for tlist, fname in zip(lists, ['raw.tl', 'cell.tl', 'diap.tl', 'cntc.tl', 'flr1.tl', 'flr2.tl']):\n",
    "        fpath = os.path.join(info_path, fname)\n",
    "        with open(fpath, 'wt') as f:\n",
    "            f.write(tlist)\n",
    "            \n",
    "    fpath = os.path.join(info_path, 'map.tm')\n",
    "    with open(fpath, 'wt') as f:\n",
    "        f.write(tmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:09.893000Z",
     "start_time": "2023-08-24T11:30:09.882000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen merging bat file\n",
    "prog_tal = proc_bin_path + 'proc_iv.bat'\n",
    "\n",
    "\n",
    "ds_id_to_idx = {i:idx for idx, i in enumerate(datasets_ids)}\n",
    "\n",
    "\n",
    "for idx in tiles_info:\n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd '+proc_bin_path+'\\n'\n",
    "    \n",
    "    tile_idx = ds_id_to_idx[tiles_info[idx][1][0]]\n",
    "    n_tf = nums[tile_idx]\n",
    "    cmd += 'call ' + prog_tal + ' %03d' % idx  + ' %d' % n_tf + ' %d' % len(datasetsf_tmplts[tile_idx])\n",
    "    cmd += '\\n'\n",
    "    cmd += 'popd\\n'\n",
    "\n",
    "    bat_file = datasets_path + '%03d/'%idx + 'merge.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:10.912000Z",
     "start_time": "2023-08-24T11:30:10.900000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen all merging bat file\n",
    "cmd = '@echo off\\n'\n",
    "\n",
    "for ds_idx in tiles_info:\n",
    "    cmd += 'call %03d'%ds_idx+'\\\\merge.bat \\n'\n",
    "bat_file = datasets_path + 'merge_' + str(merged_tiles_ids) + '.bat'\n",
    "with open(bat_file, 'wt') as f:\n",
    "    f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:11.808000Z",
     "start_time": "2023-08-24T11:30:11.803000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if auto_proc:\n",
    "    start_remote_job(datasets_path, bat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cell segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:13.820000Z",
     "start_time": "2023-08-24T11:30:13.814000Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen segmentation bat file\n",
    "prog_seg = proc_bin_path + 'proc_ds_flr_n.bat'\n",
    "\n",
    "for i, idx in enumerate(datasets_ids+merged_tiles_ids):\n",
    "    test_idx = i if idx not in merged_tiles_ids else ds_id_to_idx[tiles_info[idx][1][0]]\n",
    "    has_flour = fluo_present[test_idx]\n",
    "    \n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd '+proc_bin_path+'\\n'\n",
    "    cmd += 'call ' + prog_seg + ' %03d' % idx  + ' %d' % nums[test_idx]+ ' %d' % len(datasetsf_tmplts[test_idx])\n",
    "    cmd += '\\n'\n",
    "    cmd += 'popd\\n'\n",
    "\n",
    "    bat_file = datasets_path + '%03d/'%idx + 'segment.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:14.458000Z",
     "start_time": "2023-08-24T11:30:14.453000Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gen_proc_bat(ds_ids_list, datasets_path, script_name):\n",
    "    cmd = '@echo off\\n'\n",
    "\n",
    "    for ds_idx in ds_ids_list:\n",
    "        cmd += 'call %03d'%ds_idx+f'\\\\{script_name}.bat \\n'\n",
    "    \n",
    "    if len(ds_ids_list) == 0:\n",
    "        return None\n",
    "    first_last = str(ds_ids_list[0])\n",
    "    if len(ds_ids_list)>1:\n",
    "        first_last += '-'+str(ds_ids_list[-1])\n",
    "\n",
    "    bat_file = datasets_path + f'{script_name}_[' + first_last + '].bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)\n",
    "        \n",
    "    return bat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:20.886000Z",
     "start_time": "2023-08-24T11:30:20.883000Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gen_segm_bat(ds_ids_list, datasets_path):\n",
    "    return gen_proc_bat(ds_ids_list, datasets_path, script_name='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:21.599000Z",
     "start_time": "2023-08-24T11:30:21.595000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gen all segmentation bat file\n",
    "bat_file_all_separate = gen_segm_bat(datasets_ids, datasets_path)\n",
    "bat_file_all_m = gen_segm_bat(merged_tiles_ids, datasets_path)\n",
    "bat_file_all_non_m = gen_segm_bat(all_non_tile_dataset_ids, datasets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:28.349000Z",
     "start_time": "2023-08-24T11:30:28.347000Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# process tiles\n",
    "# if auto_proc:\n",
    "#     start_remote_job(datasets_path, bat_file_all_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:30:29.202000Z",
     "start_time": "2023-08-24T11:30:29.199000Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait merging and segmnet merged\n",
    "if auto_proc:\n",
    "    start_remote_job(datasets_path, bat_file_all_non_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:33:32.855000Z",
     "start_time": "2023-08-24T11:30:32.318000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait merging and segmnet merged\n",
    "if auto_proc:\n",
    "    start_remote_job(datasets_path, bat_file_all_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:33:32.870000Z",
     "start_time": "2023-08-24T11:33:32.858000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_tiles_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:33:32.895000Z",
     "start_time": "2023-08-24T11:33:32.873000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_non_tile_dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T11:33:32.911000Z",
     "start_time": "2023-08-24T11:33:32.901000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_end_t = timer()\n",
    "print(f'notebook run time: {(nb_end_t - nb_start_t):.2f} sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
