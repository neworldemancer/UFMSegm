{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "   Copyright 2015-2023, University of Bern, Laboratory for High Energy Physics and Theodor Kocher Institute, M. Vladymyrov\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Libs & Fns"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:53:50.384407Z",
     "start_time": "2024-11-09T17:53:50.367418Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import shutil"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-11-09T17:53:50.966972Z",
     "start_time": "2024-11-09T17:53:50.952979Z"
    }
   },
   "source": [
    "def create_ds_dir(idx, ds_name, datasets_path):\n",
    "    path = os.path.join(datasets_path, '%03d'%idx)\n",
    "    if os.path.exists(path):\n",
    "        return False\n",
    "    else:\n",
    "        path = os.path.join(path, ds_name)\n",
    "        os.makedirs(path, exist_ok=False)\n",
    "        return True"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-11-09T17:53:52.070525Z",
     "start_time": "2024-11-09T17:53:52.049539Z"
    }
   },
   "source": [
    "def make_record_info(idx, ds_name, datasets_path, tile = None):\n",
    "    path = os.path.join(datasets_path, 'info')\n",
    "    with open(path, 'at') as f:\n",
    "        s = '%03d - %s' % (idx, ds_name) + (', tile %d' % tile if tile is not None else '') + '\\n'\n",
    "        f.write(s)\n",
    "    f.close()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [
     0,
     83
    ],
    "ExecuteTime": {
     "end_time": "2024-11-09T17:53:52.321597Z",
     "start_time": "2024-11-09T17:53:52.298614Z"
    }
   },
   "source": [
    "def ds_info(ds_path):\n",
    "    \n",
    "    list_files = sorted([n for n in os.listdir(ds_path) if '.tif' in n])\n",
    "    \n",
    "    \n",
    "    file_names = [fn.replace('.tif', '') for fn in list_files if ('t' in fn and '_' in fn)] # only formatted, timelapse\n",
    "    \n",
    "    list_sfx = [fn.split('_')[-1] for fn in file_names]\n",
    "    \n",
    "    name_tmpls = ['_'.join(fn.split('_')[:-1]) for fn in file_names if '_' in fn]\n",
    "    \n",
    "    if len(name_tmpls) == 0:\n",
    "        raise FileNotFoundError('files with name \"<xxx>_<xx>t%d<xx>.tif not found\"')\n",
    "    \n",
    "    name_tmpl = name_tmpls[0]\n",
    "    \n",
    "    last_sfx = list_sfx[-1]\n",
    "    has_ch =  'c' in last_sfx\n",
    "    has_tl =  'm' in last_sfx\n",
    "    has_ps =  's' in last_sfx  # positions\n",
    "    \n",
    "    if has_ps:\n",
    "        last_sfx_trunc_s = last_sfx.replace('s', '')\n",
    "        \n",
    "        n_ps_s, res_sfx = last_sfx_trunc_s.split('t')\n",
    "        n_ps = int(n_ps_s)\n",
    "        \n",
    "        if has_ch:\n",
    "            n_t_s, ch_tl_s = res_sfx.split('c')\n",
    "            n_t = int(n_t_s)\n",
    "\n",
    "            if has_tl:\n",
    "                n_ch, n_tl = [int(s) for s in ch_tl_s.split('m')]\n",
    "            else:\n",
    "                n_ch = int(ch_tl_s)\n",
    "                n_tl = 1\n",
    "        else:\n",
    "            if has_tl:\n",
    "                n_t, n_tl = [int(s) for s in res_sfx.split('m')]\n",
    "            else:\n",
    "                n_t = int(res_sfx)\n",
    "                n_tl = 1\n",
    "            n_ch = 1\n",
    "    else:\n",
    "        n_ps = 1\n",
    "        last_sfx_trunc_t = last_sfx.replace('t', '')\n",
    "        if has_ch:\n",
    "            n_t_s, ch_tl_s = last_sfx_trunc_t.split('c')\n",
    "            n_t = int(n_t_s)\n",
    "\n",
    "            if has_tl:\n",
    "                n_ch, n_tl = [int(s) for s in ch_tl_s.split('m')]\n",
    "            else:\n",
    "                n_ch = int(ch_tl_s)\n",
    "                n_tl = 1\n",
    "        else:\n",
    "            if has_tl:\n",
    "                n_t, n_tl = [int(s) for s in last_sfx_trunc_t.split('m')]\n",
    "            else:\n",
    "                n_t = int(last_sfx_trunc_t)\n",
    "                n_tl = 1\n",
    "            n_ch = 1\n",
    "        \n",
    "    \n",
    "    tmpl_ps = 's%0' + '%d' % len('%d' % n_tl) + 'd'\n",
    "    tmpl_t = 't%0' + '%d' % len('%d' % n_t) + 'd'\n",
    "    tmpl_ch = 'c%0' + '%d' % len('%d' % n_ch) + 'd'\n",
    "    tmpl_tl = 'm%0' + '%d' % len('%d' % n_tl) + 'd'\n",
    "    \n",
    "    return {'n_ps' : n_ps, \n",
    "            'n_t' : n_t, \n",
    "            'n_ch' :n_ch, \n",
    "            'n_tl' : n_tl, \n",
    "            'has_ps' :has_ps, \n",
    "            'has_ch' :has_ch, \n",
    "            'has_tl' :has_tl,\n",
    "            'tmpl_ps' :tmpl_ps,\n",
    "            'tmpl_t' :tmpl_t,\n",
    "            'tmpl_ch' :tmpl_ch,\n",
    "            'tmpl_tl' :tmpl_tl,\n",
    "            'name_tmpl':name_tmpl\n",
    "           }\n",
    "\n",
    "def get_file_name(path, inf_dict, ps, t, ch, tile):\n",
    "    sfx = '_'\n",
    "    \n",
    "    if inf_dict['has_ps']:\n",
    "        sfx += inf_dict['tmpl_ps'] % (ps+1)\n",
    "    sfx += inf_dict['tmpl_t'] % (t+1)\n",
    "    if inf_dict['has_ch']:\n",
    "        sfx += inf_dict['tmpl_ch'] % (ch+1)\n",
    "    if inf_dict['has_tl']:\n",
    "        sfx += inf_dict['tmpl_tl'] % (tile+1)\n",
    "\n",
    "    name = inf_dict['name_tmpl'] + sfx + '.tif'\n",
    "    fn = os.path.join(path, name)\n",
    "    return fn"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-11-09T17:55:15.678176Z",
     "start_time": "2024-11-09T17:55:15.652193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_segmentation_datasets(datasets_path, datasets_names, start_ds_idx, time_subsample_min_t = 210):  # if n_t >time_subsample_min_t - take every second frame\n",
    "    for ds_names in datasets_names:\n",
    "        if isinstance(ds_names, str):\n",
    "            ds_names = [(0, ds_names, 0)]\n",
    "            \n",
    "        # 1. get info for all\n",
    "        copy_struct = []  # 1 element per ds: (ds_dir_name, info, n_before, n_after)\n",
    "        \n",
    "        all_nt = []\n",
    "        for item in ds_names:\n",
    "            n_copy_before, ds_path, n_copy_after = item\n",
    "            inf = ds_info(ds_path)\n",
    "            copy_struct.append((ds_path, inf, n_copy_before, n_copy_after))\n",
    "            all_nt.append(inf['n_t'])\n",
    "            \n",
    "        subsample_fact = 2 if max(all_nt)>time_subsample_min_t else 1\n",
    "        # print(subsample_fact, copy_struct, '\\n')\n",
    "        # continue\n",
    "            \n",
    "        # check validity: all mush have same format\n",
    "        for key in ['n_ps', 'n_ch', 'n_tl', 'has_ps', 'has_ch', 'has_tl']:\n",
    "            el0 = copy_struct[0][1][key]\n",
    "            for struct in copy_struct[1:]:\n",
    "                assert(struct[1][key] == el0)\n",
    "        \n",
    "        n_t_out = 0  # num output timeframes\n",
    "        for struct in copy_struct:\n",
    "            n_t_out += struct[2] + struct[3] + (struct[1]['n_t'] // subsample_fact)\n",
    "            \n",
    "        inf0 = copy_struct[0][1]\n",
    "        n_ch = inf0['n_ch']\n",
    "        \n",
    "        n_ps_i = inf0['n_ps']\n",
    "        n_tl_i = inf0['n_tl']\n",
    "        \n",
    "        n_tl = n_ps_i * n_tl_i\n",
    "        \n",
    "        has_ch = inf0['has_ch']\n",
    "        has_tl = inf0['has_ps'] or inf0['has_tl']\n",
    "        \n",
    "        out_ds_name_general = '_'.join([struct[1]['name_tmpl'] for struct in copy_struct])\n",
    "        \n",
    "        tmpl_t = 't%0' + '%d' % len('%d' % n_t_out) + 'd'\n",
    "        tmpl_ch = 'c%0' + '%d' % len('%d' % n_ch) + 'd'\n",
    "        tmpl_tl = 'm%0' + '%d' % len('%d' % 1) + 'd'\n",
    "        \n",
    "        oinf = {'n_ps' : 1, \n",
    "                'n_t' : n_t_out, \n",
    "                'n_ch' :n_ch, \n",
    "                'n_tl' : 1, \n",
    "                'has_ps' :False,\n",
    "                'has_ch' :has_ch, \n",
    "                'has_tl' :False,\n",
    "                'tmpl_t' :tmpl_t,\n",
    "                'tmpl_ch' :tmpl_ch,\n",
    "                'tmpl_tl' :tmpl_tl,\n",
    "                'name_tmpl':out_ds_name_general\n",
    "               }\n",
    "        \n",
    "        # create dirs and fill info file\n",
    "        tile_to_idx = {}\n",
    "        tile_ds_name = {}\n",
    "        \n",
    "        \n",
    "        creation_ok = True\n",
    "        for tl in range(n_tl):\n",
    "            idx = start_ds_idx + tl\n",
    "            tile_to_idx[tl] = idx\n",
    "    \n",
    "            out_ds_name = out_ds_name_general + ('_tile%d' % (tl+1) if has_tl else '')\n",
    "            tile_ds_name[tl] = out_ds_name\n",
    "    \n",
    "            if not create_ds_dir(idx, out_ds_name, datasets_path):\n",
    "                print('dataset with idx', idx, 'already exists. please check manually. Aborting.')\n",
    "                creation_ok = False\n",
    "                break\n",
    "        if not creation_ok:\n",
    "            break\n",
    "            \n",
    "        start_ds_idx += n_tl\n",
    "    \n",
    "    \n",
    "        for tl in range(n_tl):\n",
    "            idx = tile_to_idx[tl]\n",
    "            out_ds_name = tile_ds_name[tl]\n",
    "            make_record_info(idx, out_ds_name, datasets_path, (tl+1) if has_tl else None)\n",
    "    \n",
    "            ods_path = os.path.join(datasets_path, '%03d' % idx)\n",
    "            with open(os.path.join(ods_path, 'info.txt'), 'wt') as f:\n",
    "                f.write(out_ds_name)\n",
    "    \n",
    "        \n",
    "        for ps_i in range(n_ps_i):\n",
    "            for tl_i in range(n_tl_i):\n",
    "                tl = ps_i * n_tl_i + tl_i\n",
    "                \n",
    "                idx = tile_to_idx[tl]\n",
    "                out_ds_name = tile_ds_name[tl]\n",
    "                oinf['name_tmpl'] = out_ds_name\n",
    "    \n",
    "                ods_path = os.path.join(datasets_path, '%03d' % idx, out_ds_name)\n",
    "                \n",
    "                block_boundaries = []\n",
    "                for ch in range(n_ch):\n",
    "                    t_o = 0\n",
    "                    for struct in copy_struct:\n",
    "                        in_path, inf, copy_before, copy_after = struct\n",
    "                        n_t_i = inf['n_t']\n",
    "                        for i in range(copy_before):\n",
    "                            t_i = 0\n",
    "                            i_file = get_file_name(in_path,   inf, ps_i, t_i, ch, tl_i)\n",
    "                            o_file = get_file_name(ods_path, oinf,   -1, t_o, ch, 0)\n",
    "                            shutil.copy(i_file, o_file)\n",
    "                            t_o += 1\n",
    "    \n",
    "                        for t_i in range(n_t_i//subsample_fact):\n",
    "                            i_file = get_file_name(in_path,   inf, ps_i, t_i*subsample_fact, ch, tl_i)\n",
    "                            o_file = get_file_name(ods_path, oinf,   -1, t_o, ch, 0)\n",
    "                            shutil.move(i_file, o_file)\n",
    "                            t_o += 1\n",
    "    \n",
    "                        for i in range(copy_after):\n",
    "                            i_file = get_file_name(ods_path, oinf, ps_i, t_o-1, ch, tl_i)\n",
    "                            o_file = get_file_name(ods_path, oinf,   -1, t_o,   ch, 0)\n",
    "                            shutil.copy(i_file, o_file)\n",
    "                            t_o += 1\n",
    "                            \n",
    "                        if ch==0:\n",
    "                            begin = 0 if len(block_boundaries)==0 else block_boundaries[-1][1]\n",
    "                            end = t_o\n",
    "                            block_boundaries.append([begin, end])\n",
    "                            \n",
    "                block_info_path = os.path.join(datasets_path, '%03d' % idx, 'block_info.txt')\n",
    "                with open(block_info_path, 'wt') as f:\n",
    "                    txt = '|'.join([' '.join([str(bi) for bi in b]) for b in block_boundaries])\n",
    "                    f.write(txt)\n",
    "    \n",
    "        for item in ds_names:\n",
    "            n_copy_before, ds_path, n_copy_after = item\n",
    "            #shutil.rmtree(ds_path)\n",
    "    \n",
    "    return start_ds_idx"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets info"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:53:55.314173Z",
     "start_time": "2024-11-09T17:53:55.298159Z"
    }
   },
   "source": [
    "start_ds_idx = 0"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-11-09T17:53:57.978107Z",
     "start_time": "2024-11-09T17:53:57.960132Z"
    }
   },
   "source": [
    "datasets_names = []\n",
    "path = '../raw_data/yyyy.mm.dd'\n",
    "path = r'g:\\IVFCA\\Dixy\\run1\\raw\\bk'\n",
    "path = os.path.abspath(path)\n",
    "for p2 in sorted(os.listdir(path)):\n",
    "        path3 = os.path.join(path, p2)\n",
    "        datasets_names.append([[0, path3, 0]])\n",
    "        \n",
    "# multiple sequences can be joiined, e..g if differernt experimental parts are saved in different czi. paddings can be added:\n",
    "# datasets_names = [\n",
    "#     [[0, 'path_part_1', 1],\n",
    "#      [1, 'path_part_1', 0]]\n",
    "# ]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:53:59.617243Z",
     "start_time": "2024-11-09T17:53:59.598255Z"
    }
   },
   "source": [
    "datasets_names"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 'g:\\\\IVFCA\\\\Dixy\\\\run1\\\\raw\\\\bk\\\\FC2_Dotarem', 0]],\n",
       " [[0, 'g:\\\\IVFCA\\\\Dixy\\\\run1\\\\raw\\\\bk\\\\FC3_Untreated', 0]],\n",
       " [[0, 'g:\\\\IVFCA\\\\Dixy\\\\run1\\\\raw\\\\bk\\\\FC4_Dotarem', 0]],\n",
       " [[0, 'g:\\\\IVFCA\\\\Dixy\\\\run1\\\\raw\\\\bk\\\\FC8_Untreated', 0]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:54:05.781791Z",
     "start_time": "2024-11-09T17:54:05.767799Z"
    }
   },
   "source": [
    "datasets_path = '../datasets_seg/'"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:04:33.228239Z",
     "start_time": "2024-11-09T17:55:18.407583Z"
    }
   },
   "source": [
    "start_ds_idx = create_segmentation_datasets(datasets_path, \n",
    "                                            datasets_names,\n",
    "                                            start_ds_idx)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
