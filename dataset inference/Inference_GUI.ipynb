{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Inference of dataset XXX by XX"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Copyright 2015-2023, University of Bern, Laboratory for High Energy Physics and Theodor Kocher Institute, M. Vladymyrov\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.214566Z",
     "start_time": "2024-10-30T16:12:09.291504Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\development\\Anaconda3\\envs\\tf_1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\development\\Anaconda3\\envs\\tf_1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\development\\Anaconda3\\envs\\tf_1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\development\\Anaconda3\\envs\\tf_1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\development\\Anaconda3\\envs\\tf_1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\development\\Anaconda3\\envs\\tf_1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "from functools import partial\n",
    "\n",
    "sys.path.append('../model training')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import interp1d as interp\n",
    "\n",
    "from time import time as timer\n",
    "from time import sleep\n",
    "import shutil\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "from threading import Thread\n",
    "import glob\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout, HBox, VBox, Label, Box, Text, Dropdown, Button, Output, HTML\n",
    "\n",
    "from utils import net_utils as nu\n",
    "from utils import imgio as iio\n",
    "from utils import predictor as pr\n",
    "from utils.histnorm import NormHist\n",
    "import tensorflow as tf\n",
    "\n",
    "import dataclasses\n",
    "import json\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.233517Z",
     "start_time": "2024-10-30T16:12:12.216562Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# make proc config dataclass\n",
    "@dataclasses.dataclass\n",
    "class ProcConfig:\n",
    "    next_dataset_id: int = 0\n",
    "    dataset_id_first_run: int = -1\n",
    "    dataset_id_last_run: int = -1\n",
    "    \n",
    "    datasets_path: str = os.path.abspath('../../datasets_seg')+os.path.sep\n",
    "    models_path: str = os.path.abspath('../../trained models')+os.path.sep\n",
    "    ref_ds_path: str = models_path+'ref datasets'+os.path.sep\n",
    "    datasets_path_proc: str = 'D:\\\\UFMTrack\\\\datasets_seg\\\\' # 'Q:\\\\' #\n",
    "    VF_root: str = 'C:\\\\VivoFollow\\\\'\n",
    "    rds_id: int = 7\n",
    "    \n",
    "    fast_model_itr: int = 30001\n",
    "    main_model_itr: int = 35001\n",
    "    \n",
    "    fast_model_name: str = 'model_BBB_BN_TV_FCN4_HN_CDC_2D_2021.04.17_22-54'   # 2D, 1 tf 2021 best \n",
    "    main_model_name: str = 'model_BBB_BN_TV_FCNx16_HN_CDC_2021.07.06_16-25'    # 3D, 2021 best\n",
    "\n",
    "    cuda_dev_ids: list = dataclasses.field(default_factory=lambda: [0])  # [0, 1, 2, 3, 4, 5, 6, 7] # list of available GPUs\n",
    "    \n",
    "    \n",
    "@dataclasses.dataclass\n",
    "class PathConfig:\n",
    "    datasets_path: str = \"\"\n",
    "    datasets_path_proc: str = \"\"\n",
    "    models_path: str = \"\"\n",
    "    ref_ds_path: str = \"\"\n",
    "    VF_root: str = \"\"\n",
    "    proc_bin_path: str = \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.261353Z",
     "start_time": "2024-10-30T16:12:12.242492Z"
    },
    "code_folding": [
     2,
     10
    ]
   },
   "outputs": [],
   "source": [
    "# methods for loading and storing the config to file\n",
    "_cfg_filename = 'proc_cfg.json'\n",
    "def load_proc_cfg(cfg_path=None):\n",
    "    cfg_path = cfg_path or os.path.join(os.path.abspath(os.path.curdir), _cfg_filename)\n",
    "    if os.path.exists(cfg_path):\n",
    "        with open(cfg_path, 'rt') as f:\n",
    "            cfg = ProcConfig(**json.load(f))\n",
    "    else:\n",
    "        cfg = ProcConfig()\n",
    "    return cfg\n",
    "    \n",
    "def save_proc_cfg(cfg, cfg_path=None):\n",
    "    cfg_path = cfg_path or os.path.join(os.path.abspath(os.path.curdir), _cfg_filename)\n",
    "    with open(cfg_path, 'wt') as f:\n",
    "        # human-readable, 4 spaces indentation\n",
    "        json.dump(dataclasses.asdict(cfg), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.291496Z",
     "start_time": "2024-10-30T16:12:12.263348Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_tiled_groups(datasets_is_tile, \n",
    "                     datasets_tile_group_id, datasets_tile_idx,\n",
    "                     datasets_ids, datasets_hw, merged_ds_start_idx,\n",
    "                     overlap_frac=0.2\n",
    "                    ):\n",
    "    all_tile_dataset_ids = []\n",
    "    all_non_tile_dataset_ids = []\n",
    "    \n",
    "    tiles_info = {} #id: [[ny, nx, tile_dx, tile_dy], list_of_tile_dataset_ids],\n",
    "    \n",
    "    curr_group_id = None\n",
    "    curr_group_ds_idxs = []\n",
    "    \n",
    "    def get_group_size_ofs(group_hw, n_tiles):\n",
    "        group_hw = np.array(group_hw)\n",
    "        # print(group_hw)\n",
    "        mean_hw = group_hw.mean(axis=0)\n",
    "        \n",
    "        deviation_hw = np.abs(mean_hw-group_hw) / mean_hw\n",
    "        incompatible_hw = deviation_hw>0.01 # 1% difference is too much\n",
    "        \n",
    "        assert not np.any(incompatible_hw), 'size of time tiles are too different'\n",
    "        # print(group_hw, mean_hw, incompatible_hw)\n",
    "        \n",
    "        h, w = mean_hw\n",
    "        overlap = w * overlap_frac  # 10% of width\n",
    "        tile_dx = int(w - overlap)\n",
    "        tile_dy = int(h - overlap)\n",
    "        \n",
    "        # simple rules for identifying tile configuration:\n",
    "        ny_nx = []\n",
    "        ny_p_nx = []\n",
    "        for ny in range(1, 1+int(np.floor(np.sqrt(n_tiles)))):\n",
    "            nx = n_tiles // ny\n",
    "            res = n_tiles - ny * nx\n",
    "            if res == 0:\n",
    "                ny_nx.append([ny, nx])\n",
    "                ny_p_nx.append(ny+nx)\n",
    "        idx = np.argmin(ny_p_nx)\n",
    "        ny, nx = ny_nx[idx]\n",
    "        \n",
    "        #print(ny_nx)\n",
    "        #print(ny_nx[idx])\n",
    "\n",
    "        return [ny, nx, tile_dx, tile_dy]\n",
    "    \n",
    "    def fill_group(curr_group_ds_idxs, datasets_ids, datasets_hw,\n",
    "                   all_tile_dataset_ids, all_non_tile_dataset_ids,\n",
    "                   tiles_info, group_idx):\n",
    "\n",
    "        if len(curr_group_ds_idxs) < 2:\n",
    "            # cancel group\n",
    "            all_non_tile_dataset_ids.extend([datasets_ids[idx] for idx in curr_group_ds_idxs])\n",
    "        else:\n",
    "            groups_ds_ids = [datasets_ids[idx] for idx in curr_group_ds_idxs]\n",
    "            print(f'processing group {group_idx}, ds ids: {groups_ds_ids}')\n",
    "            all_tile_dataset_ids.extend(groups_ds_ids)\n",
    "\n",
    "            n_tiles = len(groups_ds_ids)\n",
    "            \n",
    "            group_hw = [datasets_hw[idx] for idx in curr_group_ds_idxs]\n",
    "            \n",
    "            size_ofs = get_group_size_ofs(group_hw, n_tiles)\n",
    "            tiles_info[merged_ds_start_idx + group_idx[0]] = [size_ofs, groups_ds_ids]\n",
    "\n",
    "            group_idx[0] += 1\n",
    "    \n",
    "    \n",
    "    group_idx = [0]  # in a list to be modified inside fill_group function\n",
    "    for idx, (is_tile, group_id, tile_idx, ds_id) in enumerate(zip(datasets_is_tile, \n",
    "                                                                   datasets_tile_group_id, \n",
    "                                                                   datasets_tile_idx, \n",
    "                                                                   datasets_ids)):\n",
    "        group_close = (curr_group_id is not None) and (not is_tile or group_id != curr_group_id)\n",
    "        \n",
    "        if group_close:\n",
    "            fill_group(curr_group_ds_idxs, datasets_ids, datasets_hw,\n",
    "                       all_tile_dataset_ids, all_non_tile_dataset_ids,\n",
    "                       tiles_info, group_idx)\n",
    "            \n",
    "            curr_group_id = None\n",
    "            curr_group_ds_idxs = []\n",
    "                \n",
    "        if not is_tile:\n",
    "            all_non_tile_dataset_ids.append(ds_id)\n",
    "        else:\n",
    "            assert (curr_group_id==group_id or curr_group_id is None)\n",
    "            curr_group_id = group_id\n",
    "            assert tile_idx==len(curr_group_ds_idxs)\n",
    "            curr_group_ds_idxs.append(idx)\n",
    "            \n",
    "    \n",
    "    if curr_group_id is not None:\n",
    "        fill_group(curr_group_ds_idxs, datasets_ids, datasets_hw, \n",
    "                       all_tile_dataset_ids, all_non_tile_dataset_ids,\n",
    "                       tiles_info, group_idx)\n",
    "        \n",
    "    return all_non_tile_dataset_ids, all_tile_dataset_ids, tiles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.307456Z",
     "start_time": "2024-10-30T16:12:12.292521Z"
    },
    "code_folding": [
     0,
     1,
     20,
     23,
     33
    ]
   },
   "outputs": [],
   "source": [
    "class PredictorMT:\n",
    "    def __init__(self, mod_path, mod_itr,\n",
    "                 gpu_ids,\n",
    "                 batch_sz, input_sz,\n",
    "                 io_map,\n",
    "                 z_border=2  # one side\n",
    "                ):\n",
    "        self.z_border = z_border\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.n_thr = len(self.gpu_ids)\n",
    "        \n",
    "        self.predictors = [pr.Predictor(mod_path, mod_itr,\n",
    "                                        device_id=None,\n",
    "                                        device_ids=gpu_ids,\n",
    "                                        gpuid=i,\n",
    "                                        batch_sz=batch_sz, input_sz=input_sz,\n",
    "                                        in_out_dict=io_map,\n",
    "                                       ) for i, dev_id in enumerate(gpu_ids)]\n",
    "        self.res = []\n",
    "        \n",
    "    def normalize_stack(self, stack, norm_stack, normalization_percentile_range):\n",
    "        return self.predictors[0].normalize_stack(stack, norm_stack, normalization_percentile_range)\n",
    "    \n",
    "    def split_stack(self, stack):\n",
    "        n_z = len(stack)\n",
    "        n_z_chunk = (n_z + self.n_thr - 1) // self.n_thr\n",
    "        \n",
    "        begin_end_pairs = [[max(0, i*n_z_chunk-self.z_border), \n",
    "                            min(n_z, (i+1)*n_z_chunk+self.z_border)] for i in range(self.n_thr)]\n",
    "        \n",
    "        chunks = [stack[b:e] for b,e in begin_end_pairs]\n",
    "        return chunks\n",
    "    \n",
    "    def merge_result(self, stacks):\n",
    "        #self.z_border\n",
    "        \n",
    "        cropped_overlap = [\n",
    "                            s[0 if i==0 else self.z_border:\n",
    "                              len(s) if i==(self.n_thr-1) else -self.z_border\n",
    "                             ]\n",
    "                            for i, s in enumerate(stacks)\n",
    "                          ]\n",
    "        \n",
    "        res = np.concatenate(cropped_overlap, axis=0)\n",
    "        return res\n",
    "    \n",
    "    def _predict_image_stack(self, stack_norm, margin, keep_edge, edge_size, thr_idx):\n",
    "        pred = self.predictors[thr_idx]\n",
    "        res = pred.predict_image_stack(stack_norm, margin, keep_edge, edge_size)\n",
    "        self.res[thr_idx] = res\n",
    "        \n",
    "    def predict_image_stack(self, stack_norm, margin, keep_edge=True, edge_size=0):\n",
    "        # split stack\n",
    "        chunks = self.split_stack(stack_norm)\n",
    "        \n",
    "        self.res = [None for i in range(self.n_thr)]\n",
    "        \n",
    "        # make threads\n",
    "        threads = [Thread(target = self._predict_image_stack,\n",
    "                          args = (chunks[i], margin, keep_edge, edge_size, i)\n",
    "                         ) for i in range(self.n_thr)]\n",
    "        \n",
    "        # run all\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        \n",
    "        # wait all\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        merged = self.merge_result(self.res)\n",
    "        # merge output\n",
    "        # processed.shape == (192, 1077, 1405, 3), i.e. plain np tensor\n",
    "        \n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.322818Z",
     "start_time": "2024-10-30T16:12:12.309450Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def proc_path(path, pcfg: PathConfig):\n",
    "    return path.replace(pcfg.datasets_path, pcfg.datasets_path_proc).replace('/','\\\\').replace('%','%%')\n",
    "\n",
    "def make_genmask_bat(ds_idx, stck_tmpl, num, pcfg: PathConfig):\n",
    "    # gen background mask generation bat file. Alignment pipeline is used\n",
    "    prog = pcfg.proc_bin_path+'TimeFramesAligner_64.exe '\n",
    "    cfg = f'-cfg:{pcfg.proc_bin_path}TFAligner_bg_mask.cfg '\n",
    "    \n",
    "\n",
    "    mask_tmpl = pcfg.datasets_path +'%03d/'%ds_idx+'pred_for_algn/'+'img_%03d.png'\n",
    "    tgt_dir = pcfg.datasets_path +'%03d/'%ds_idx+'imgs_aligned_all/'\n",
    "    s_t = proc_path(stck_tmpl, pcfg)\n",
    "    m_t = proc_path(mask_tmpl, pcfg)\n",
    "    tgd = proc_path(tgt_dir, pcfg)\n",
    "    \n",
    "    cmd = '@echo off\\n'\n",
    "    cmd = 'pushd %~dp0\\n'\n",
    "    cmd += prog+cfg\n",
    "    cmd += '-savedir:\"%s\" ' % tgd\n",
    "    cmd += '-n_itr:0 '\n",
    "    cmd += '-stack:\"%s\" ' % s_t\n",
    "    cmd += '-mask:\"%s\" ' % m_t\n",
    "    cmd += '-n_frames:%d ' %  num\n",
    "    \n",
    "    cmd += '\\n'\n",
    "    \n",
    "    cmd += 'popd\\n'\n",
    "        \n",
    "    bat_file = pcfg.datasets_path + '%03d/'%ds_idx + 'genmask.bat'\n",
    "    #print(cmd)\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.338845Z",
     "start_time": "2024-10-30T16:12:12.324813Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gen_collective_genmask(ds_idx_list, pcfg: PathConfig):\n",
    "    \"\"\"\n",
    "    Generated all genmask bat file: executes individual genmask\n",
    "    returns: full path to created batfile\n",
    "    \"\"\"\n",
    "    #cmd = '@echo off\\n'\n",
    "    cmd = ''\n",
    "\n",
    "    for ds_idx in ds_idx_list:\n",
    "        cmd += 'call %03d'%ds_idx+'\\\\genmask.bat \\n'\n",
    "    bat_file = pcfg.datasets_path + 'genmask_' + str(ds_idx_list) + '.bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)\n",
    "        \n",
    "    return bat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.354693Z",
     "start_time": "2024-10-30T16:12:12.341837Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def start_remote_job(pcfg: PathConfig, bat_file, iteration_sleep_time=5):\n",
    "    if bat_file is None:\n",
    "        return\n",
    "    \n",
    "    itr = 0\n",
    "    while(os.path.exists(pcfg.datasets_path + 'remote.bat')):\n",
    "        if itr==0:\n",
    "            print('Waiting previous remote job to be done...')\n",
    "        itr += 1\n",
    "        sleep(iteration_sleep_time)\n",
    "    shutil.copy(bat_file, pcfg.datasets_path + 'remote.bat')\n",
    "    \n",
    "def wait_for_file(file_path, iteration_sleep_time=5, end_sleep_time=10):\n",
    "    while not os.path.exists(file_path):\n",
    "        time.sleep(iteration_sleep_time)\n",
    "\n",
    "    time.sleep(end_sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.370414Z",
     "start_time": "2024-10-30T16:12:12.356647Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def show_mdl_smpl(raw, cdc):\n",
    "    mdl=len(cdc)//2\n",
    "    _=iio.draw_samples(\n",
    "        (\n",
    "            raw[mdl,...,0],\n",
    "            cdc[mdl,...,0],\n",
    "            cdc[mdl,...,1],\n",
    "            cdc[mdl,...,2]\n",
    "        ), color_range=(0,256)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.385411Z",
     "start_time": "2024-10-30T16:12:12.371399Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gen_proc_bat(ds_ids_list, pcfg: PathConfig, script_name):\n",
    "    cmd = '@echo off\\n'\n",
    "\n",
    "    for ds_idx in ds_ids_list:\n",
    "        cmd += 'call %03d'%ds_idx+f'\\\\{script_name}.bat \\n'\n",
    "    \n",
    "    if len(ds_ids_list) == 0:\n",
    "        return None\n",
    "    first_last = str(ds_ids_list[0])\n",
    "    if len(ds_ids_list)>1:\n",
    "        first_last += '-'+str(ds_ids_list[-1])\n",
    "\n",
    "    bat_file = pcfg.datasets_path + f'{script_name}_[' + first_last + '].bat'\n",
    "    with open(bat_file, 'wt') as f:\n",
    "        f.write(cmd)\n",
    "        \n",
    "    return bat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:12:12.401372Z",
     "start_time": "2024-10-30T16:12:12.386439Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gen_segm_bat(ds_ids_list, pcfg: PathConfig):\n",
    "    return gen_proc_bat(ds_ids_list, pcfg, script_name='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T08:22:50.650720Z",
     "start_time": "2024-11-01T08:22:50.529333Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_segmentation(cfg: ProcConfig):\n",
    "    nb_start_t = timer()\n",
    "    proc_res = {'status': False, 'last_dataset_idx': -1}\n",
    "    \n",
    "    \n",
    "    # all work here\n",
    "    try:\n",
    "        # datasets\n",
    "        datasets_ids = list(range(cfg.dataset_id_first_run, cfg.dataset_id_last_run+1)) # range of tiled datasets, expected to be last folders, and 8 tiles\n",
    "        merged_ds_idx0 = datasets_ids[-1] +1\n",
    "        \n",
    "        # path from inference node\n",
    "        pcfg = PathConfig(datasets_path=cfg.datasets_path,\n",
    "                          datasets_path_proc=cfg.datasets_path_proc,\n",
    "                          models_path=cfg.models_path,\n",
    "                          ref_ds_path=cfg.ref_ds_path,\n",
    "                          VF_root=cfg.VF_root,\n",
    "                          proc_bin_path=cfg.VF_root + 'bin\\\\'\n",
    "                          )\n",
    "\n",
    "        fast_model_itr = cfg.fast_model_itr\n",
    "        main_model_itr = cfg.main_model_itr\n",
    "        \n",
    "        fast_model_name = cfg.fast_model_name \n",
    "        main_model_name = cfg.main_model_name\n",
    "        \n",
    "        mod_path_f = pcfg.models_path + '/' + fast_model_name\n",
    "        \n",
    "        mod_path = pcfg.models_path + '/' + main_model_name\n",
    "        \n",
    "        # path from win processing node. Should be consistent with cfg files.\n",
    "        # Remote processing should be avoided for performance reasons,\n",
    "        # but if used, should be within closed private network\n",
    "        \n",
    "        dev_ids = cfg.cuda_dev_ids\n",
    "        dev_id = dev_ids[0]\n",
    "        \n",
    "        auto_proc = True\n",
    "        \n",
    "        rds_id = cfg.rds_id\n",
    "        \n",
    "        \n",
    "        nums=[]\n",
    "        datasets_names = []\n",
    "        fluo_present = []\n",
    "        \n",
    "        datasets_tmplts = []\n",
    "        datasetsf_tmplts = []\n",
    "        datasets_normed = []\n",
    "        datasets_hist_normed = []\n",
    "        \n",
    "        block_boundaries = []\n",
    "        \n",
    "        datasets_is_tile = []\n",
    "        datasets_tile_idx = []\n",
    "        datasets_tile_group_id = []\n",
    "        datasets_hw = []  # image hight/width\n",
    "        \n",
    "        tile_sep = '_tile'\n",
    "        for ds_idx in datasets_ids:\n",
    "            # read info file\n",
    "            info_file_name = pcfg.datasets_path + '%03d/info.txt' % ds_idx\n",
    "            with open(info_file_name, 'rt') as f:\n",
    "                ds_name = f.readline()\n",
    "            \n",
    "            #list files in dir\n",
    "            ds_path = pcfg.datasets_path + '%03d/%s/' % (ds_idx, ds_name)\n",
    "            all_tif_files = [n for n in os.listdir(ds_path) if ds_name in n]\n",
    "            \n",
    "            all_sfx = sorted([n.replace(ds_name+'_t', '').replace('.tif', '') for n in all_tif_files])\n",
    "            last_sfx = all_sfx[-1]\n",
    "            \n",
    "            is_tile = tile_sep in ds_name\n",
    "            if is_tile:\n",
    "                grpid_idx = ds_name.split(tile_sep)\n",
    "                assert len(grpid_idx)==2, f'unexpected dataset name format: \"{ds_name}\", contains multiple \"{tile_sep}\"'\n",
    "                tile_group_id, tile_idx = grpid_idx\n",
    "                tile_idx = int(tile_idx) - 1\n",
    "            else:\n",
    "                tile_group_id, tile_idx = '', -1\n",
    "            \n",
    "            #get # channel, #time points\n",
    "            has_fluo = 'c' in last_sfx\n",
    "            \n",
    "            if has_fluo:\n",
    "                n_t_c_s = last_sfx.split('c')\n",
    "                n_t, n_c = [int(s) for s in n_t_c_s]\n",
    "            else:\n",
    "                n_c = 1\n",
    "                n_t = int(last_sfx)\n",
    "            \n",
    "            #fill tmplts, num, fluo present, \n",
    "            tmpl_t = '_t%0' + '%d' % len('%d' % n_t) + 'd'\n",
    "            tmpl_c = 'c%0' + '%d' % len('%d' % n_c if has_fluo else '1') + 'd'\n",
    "            \n",
    "            dataset_tmplt = ds_path + ds_name + tmpl_t + (tmpl_c % 1 if has_fluo else '') +'.tif'\n",
    "            datasetf_tmplts = [ds_path + ds_name + tmpl_t + (tmpl_c % ch) +'.tif' for ch in range(2, n_c+1)] if has_fluo else []\n",
    "            dataset_normed = ds_path + 'normed'\n",
    "            \n",
    "            dataset_hist_normed =  pcfg.datasets_path + '%03d/' % ds_idx + 'hist_normed/img_%03d.tif' #png\n",
    "        \n",
    "            #print(n_t, n_c, has_fluo, tmpl_t, tmpl_c, dataset_tmplt, datasetf_tmplts, dataset_normed)\n",
    "            \n",
    "            stack = iio.read_image_stack(dataset_tmplt, 1, 1)\n",
    "            hw = stack.shape[1:]\n",
    "            \n",
    "            # use num per dataset\n",
    "            nums.append(n_t)\n",
    "            datasets_names.append(ds_name)\n",
    "            \n",
    "            datasets_is_tile.append(is_tile)\n",
    "            datasets_tile_idx.append(tile_idx)\n",
    "            datasets_tile_group_id.append(tile_group_id)\n",
    "            \n",
    "            datasets_hw.append(hw)\n",
    "            \n",
    "            fluo_present.append(has_fluo)\n",
    "        \n",
    "            datasets_tmplts.append(dataset_tmplt)\n",
    "            datasetsf_tmplts.append(datasetf_tmplts)\n",
    "            datasets_normed.append(dataset_normed)\n",
    "            \n",
    "            datasets_hist_normed.append(dataset_hist_normed)\n",
    "            \n",
    "        \n",
    "            block_info_path = os.path.join(pcfg.datasets_path, '%03d' % ds_idx, 'block_info.txt')\n",
    "            if os.path.exists(block_info_path):\n",
    "                with open(block_info_path, 'rt') as f:\n",
    "                    txt = f.readline()\n",
    "            else:\n",
    "                txt = ''\n",
    "            if txt:\n",
    "                block_boundary = [[int(bi) for bi in b.split(' ')] for b in txt.split('|')]\n",
    "            else:\n",
    "                block_boundary = [[0, n_t]]\n",
    "            block_boundaries.append(block_boundary)\n",
    "            \n",
    "        al_datasets_tmplts = [pcfg.datasets_path +'%03d' % i+ '/imgs_aligned_all/raw/%03d.png' for i,n in zip(datasets_ids,datasets_names)]\n",
    "        al_datasets_normed = [pcfg.datasets_path +'%03d' % i+ '/imgs_aligned_all/normed' for i,n in zip(datasets_ids,datasets_names)]\n",
    "        \n",
    "        all_non_tile_dataset_ids, all_tile_dataset_ids, tiles_info = get_tiled_groups(datasets_is_tile, \n",
    "                                                                                      datasets_tile_group_id, datasets_tile_idx,\n",
    "                                                                                      datasets_ids, datasets_hw,\n",
    "                                                                                      merged_ds_start_idx=merged_ds_idx0,\n",
    "                                                                                      overlap_frac=0.1\n",
    "                                                                                     )        \n",
    "        \n",
    "        # load models\n",
    "        # fast model\n",
    "        \n",
    "        io_map = {'in':'stack:0', 'out':'ModelOutput:0', 'out_channels':[0,1,2]}\n",
    "        batch_sz=8\n",
    "        \n",
    "        #print('AFTER EXECUTING THIS FIRST TIME FOR A MODEL (Needs single-threaded version!) - RESTART THE KERNEL')\n",
    "        print(mod_path_f, fast_model_itr)\n",
    "        # pred_fast = pr.Predictor(mod_path_f, fast_model_itr,\n",
    "        #                          device_id=dev_id,\n",
    "        #                          batch_sz=batch_sz, input_sz=[512,512],\n",
    "        #                          in_out_dict=io_map)\n",
    "        \n",
    "        pred_fast_mt = PredictorMT(mod_path_f, fast_model_itr,\n",
    "                           gpu_ids=dev_ids,\n",
    "                           batch_sz=batch_sz, input_sz=[512,512],\n",
    "                           io_map=io_map)\n",
    "        \n",
    "        io_map = {'in':'stack:0', 'out':'ModelOutput:0', 'out_channels':[0,1,2]}\n",
    "        \n",
    "        batch_sz=1\n",
    "        \n",
    "        print(mod_path, main_model_itr)\n",
    "        # pred = pr.Predictor(mod_path, main_model_itr,\n",
    "        #                     device_id=dev_id,\n",
    "        #                     batch_sz=batch_sz, input_sz=[512,512],\n",
    "        #                     in_out_dict=io_map)\n",
    "        pred_mt = PredictorMT(mod_path, main_model_itr,\n",
    "                           gpu_ids=dev_ids,\n",
    "                           batch_sz=batch_sz, input_sz=[512,512],\n",
    "                           io_map=io_map)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for tmpl, nnrm, num in zip(datasets_tmplts, datasets_normed, nums):\n",
    "            print(tmpl)\n",
    "            stack = iio.read_image_stack(tmpl, num, 1)\n",
    "            \n",
    "            print('saving...')\n",
    "            if len(stack.shape)==4:\n",
    "                stack = stack[...,0]\n",
    "            \n",
    "            for idx, im in enumerate(stack):\n",
    "                iio.save_image(im, tmpl % (idx+1))\n",
    "            \n",
    "            stack_center = stack[:, 130:-130, 130:-130]\n",
    "            \n",
    "            print('normalizing...')\n",
    "            stack_norm = pr.Predictor.normalize_stack(stack=stack, \n",
    "                                                      norm_stack=stack_center, \n",
    "                                                      normalization_percentile_range=(2.5, 97.5))\n",
    "            print('saving normalized...')\n",
    "            np.savez(nnrm, stack_norm)\n",
    "            \n",
    "        t0 = timer()\n",
    "        for i, (nnrm, num, ds_idx, stck_tmpl) in enumerate(zip(datasets_normed, nums, datasets_ids, datasets_tmplts)):\n",
    "            # read normalized\n",
    "            stack_normf = np.load(nnrm+'.npz')\n",
    "            for stack_norm in stack_normf.values():\n",
    "                break\n",
    "        \n",
    "            # process\n",
    "            processed = pred_fast_mt.predict_image_stack(stack_norm, margin=4, edge_size=30)\n",
    "            \n",
    "            # visualize\n",
    "            # show_mdl_smpl(stack_norm, processed)\n",
    "            \n",
    "            print('saving dataset ', ds_idx, end='\\r')\n",
    "            path = pcfg.datasets_path + '%03d/'%ds_idx + '/pred_for_algn/'\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            for i, img in enumerate(processed):\n",
    "                pimg = Image.fromarray(img[..., 0])\n",
    "                pimg.save(path+ 'img_%03d.png'%i, quality=99)\n",
    "                \n",
    "            # make maskgen bat file and run if needed\n",
    "            make_genmask_bat(ds_idx, stck_tmpl, num, pcfg)\n",
    "            \n",
    "            if auto_proc:\n",
    "                bat_file = gen_collective_genmask([ds_idx], pcfg)\n",
    "                start_remote_job(pcfg, bat_file)\n",
    "        \n",
    "        if not auto_proc:\n",
    "            for ds_idx, stck_tmpl, num in zip(datasets_ids, datasets_tmplts, nums):\n",
    "                make_genmask_bat(ds_idx, stck_tmpl, num, pcfg)\n",
    "            bat_file = gen_collective_genmask(datasets_ids, pcfg)\n",
    "            \n",
    "        t1 = timer()\n",
    "        print(t1 - t0, 's')\n",
    "        \n",
    "        # wait masks done\n",
    "        i = len(datasets_tmplts) - 1\n",
    "        idx = datasets_ids[i]\n",
    "        test_file = pcfg.datasets_path +'%03d/'%idx+'imgs_aligned_all/bin_mask_bg/%03d.png' % (nums[i]-1)\n",
    "        \n",
    "        wait_for_file(test_file)\n",
    "        \n",
    "        nh = NormHist(pcfg.ref_ds_path+r'/%02d' % rds_id, dev=0)\n",
    "        \n",
    "        for i, tmpl, tmpl_hn, boundaries, num in zip(datasets_ids, datasets_tmplts, datasets_hist_normed, block_boundaries, nums):\n",
    "            print(tmpl)\n",
    "            stack = iio.read_image_stack(tmpl, num, 1)\n",
    "            if len(stack.shape)==4:\n",
    "                stack = stack[...,0]\n",
    "                \n",
    "            stack_normed_blocks = []\n",
    "            \n",
    "            mask_tmpl = pcfg.datasets_path +'%03d/'%i+'imgs_aligned_all/bin_mask_bg/%03d.png'\n",
    "        \n",
    "            \n",
    "            for idx, block_boundary in enumerate(boundaries):\n",
    "                bl_begin, bl_end = block_boundary\n",
    "                stack_src = stack[bl_begin: bl_end]\n",
    "                stack_src_ref = stack_src[-5:] if idx==0 else stack_src[:5]\n",
    "                print('normalizing block', idx, 'range', block_boundary, '...', end='\\r')\n",
    "                \n",
    "                mask_start_idx = (bl_end - 5) if idx==0 else bl_begin\n",
    "                mask_ref = iio.read_image_stack(mask_tmpl, 5, mask_start_idx)\n",
    "                if len(mask_ref.shape)==4:\n",
    "                    mask_ref = mask_ref[...,0]\n",
    "                    \n",
    "                stack_normed_block = nh.correct_stack(stack_src, stack_src_ref, mask_ref)\n",
    "                #fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                #ax[0].plot(nh.last_lut)\n",
    "                #ax[1].plot(nh.last_lut[200:])\n",
    "                #plt.show()\n",
    "                stack_normed_blocks.append(stack_normed_block)\n",
    "        \n",
    "            stack_normed_blocks = np.concatenate(stack_normed_blocks, axis=0)\n",
    "                \n",
    "            print('saving...                                     ', end='\\r')\n",
    "            os.makedirs(os.path.dirname(tmpl_hn), exist_ok=True)\n",
    "            for idx, im in enumerate(stack_normed_blocks):\n",
    "                iio.save_image(im, tmpl_hn % idx)\n",
    "                \n",
    "        # gen alignment  bat file\n",
    "        prog = pcfg.proc_bin_path+'TimeFramesAligner_64.exe '\n",
    "        cfg = f'-cfg:{pcfg.proc_bin_path}TFAligner.cfg '\n",
    "        \n",
    "        \n",
    "        rmdir_statement = lambda path: \"\"\"IF exist %s (\n",
    "          rmdir /s /q %s\n",
    "        )\\n\"\"\" % (path, path)\n",
    "        \n",
    "        for ds_idx, stck_tmpl, stck_aux, has_aux, num  in zip(datasets_ids, datasets_hist_normed, datasetsf_tmplts, fluo_present, nums):\n",
    "            mask_tmpl = pcfg.datasets_path +'%03d/'%ds_idx+'pred_for_algn/'+'img_%03d.png'\n",
    "            tgt_dir = pcfg.datasets_path +'%03d/'%ds_idx+'imgs_aligned_all/'\n",
    "            s_t = proc_path(stck_tmpl, pcfg)\n",
    "            m_t = proc_path(mask_tmpl, pcfg)\n",
    "            tgd = proc_path(tgt_dir, pcfg)\n",
    "            \n",
    "            cmd = '@echo off\\n'\n",
    "            cmd = 'pushd %~dp0\\n'\n",
    "            cmd += prog+cfg\n",
    "            cmd += '-savedir:\"%s\" ' % tgd\n",
    "            cmd += '-n_itr:3 '\n",
    "            cmd += '-stack:\"%s\" ' % s_t\n",
    "            cmd += '-raw_idx_0:0 '\n",
    "            cmd += '-mask:\"%s\" ' % m_t\n",
    "            cmd += '-n_frames:%d ' %  num\n",
    "            \n",
    "            if has_aux:\n",
    "                for aux_id, aux_tmpl in enumerate(stck_aux):\n",
    "                    s_t = proc_path(aux_tmpl, pcfg)\n",
    "                    cmd += '-stack_%d:\"%s\" ' % (aux_id, s_t)\n",
    "                    cmd += '-stack_%d_subpixel ' % aux_id\n",
    "                    cmd += '-stack_%d_start:1 ' % aux_id\n",
    "                \n",
    "            cmd += '\\n'\n",
    "            \n",
    "            cmd += rmdir_statement('\"%sraw\"'%tgd)\n",
    "            cmd += 'move \"%scorrected\" \"%sraw\"\\n' % (tgd, tgd)\n",
    "        \n",
    "            if has_aux:\n",
    "                for aux_id, _ in enumerate(stck_aux):\n",
    "                    cmd += rmdir_statement('\"%sflr%d\"'%(tgd, aux_id+1))\n",
    "                    cmd += 'move \"%scorrected_st_%d\" \"%sflr%d\"\\n' % (tgd, aux_id, tgd, aux_id+1)\n",
    "            cmd += 'popd\\n'\n",
    "                \n",
    "            bat_file = pcfg.datasets_path + '%03d/'%ds_idx + 'align.bat'\n",
    "            #print(cmd)\n",
    "            with open(bat_file, 'wt') as f:\n",
    "                f.write(cmd)\n",
    "                \n",
    "        # gen all alignments bat file\n",
    "        cmd = '@echo off\\n'\n",
    "        \n",
    "        for ds_idx in datasets_ids:\n",
    "            cmd += 'call %03d'%ds_idx+'\\\\align.bat \\n'\n",
    "            \n",
    "        first_last = str(datasets_ids[0])\n",
    "        if len(datasets_ids)>1:\n",
    "            first_last += '-'+str(datasets_ids[-1])\n",
    "        bat_file = pcfg.datasets_path + 'align_[' + first_last + '].bat'\n",
    "        with open(bat_file, 'wt') as f:\n",
    "            f.write(cmd)\n",
    "            \n",
    "        start_remote_job(pcfg, bat_file)\n",
    "        \n",
    "        # wait alignment done\n",
    "        i = len(al_datasets_tmplts) - 1\n",
    "        test_file = al_datasets_tmplts[i]%(nums[i]-1)  # last dataset, first file\n",
    "        while not os.path.exists(test_file):\n",
    "            time.sleep(5)\n",
    "            \n",
    "        time.sleep(10)\n",
    "        \n",
    "        # normalization\n",
    "        for tmpl, nnrm, num in zip(al_datasets_tmplts, al_datasets_normed, nums):\n",
    "            print(tmpl)\n",
    "            stack = iio.read_image_stack(tmpl, num)\n",
    "            stack_center = stack[:, 130:-130, 130:-130]\n",
    "            stack_norm = pr.Predictor.normalize_stack(stack=stack, \n",
    "                                                      norm_stack=stack_center, \n",
    "                                                      normalization_percentile_range=(2.5, 97.5)\n",
    "                                                     )\n",
    "            np.savez(nnrm, stack_norm)\n",
    "            \n",
    "        # Prediction by aligned\n",
    "        t0 = timer()\n",
    "        for i, (nnrm, ds_idx) in enumerate(zip(al_datasets_normed, datasets_ids)):\n",
    "            # load normalized\n",
    "            stack_normf = np.load(nnrm+'.npz')\n",
    "            for stack_norm in stack_normf.values():\n",
    "                break\n",
    "            # process\n",
    "            processed = pred_mt.predict_image_stack(stack_norm, margin=8, edge_size=60)\n",
    "            \n",
    "            #save processed\n",
    "            print(ds_idx, end='\\r')\n",
    "            pref = os.path.join(pcfg.datasets_path,'%03d'%ds_idx)\n",
    "            path_cell = os.path.join(pref,'pred_cdc', 'cell')\n",
    "            path_diap = os.path.join(pref,'pred_cdc', 'diap')\n",
    "            path_cntr = os.path.join(pref,'pred_cdc', 'cntr')\n",
    "            path_cntC = os.path.join(pref,'pred_cdc', 'cntC')\n",
    "            \n",
    "            os.makedirs(path_cell, exist_ok=True)\n",
    "            os.makedirs(path_diap, exist_ok=True)\n",
    "            os.makedirs(path_cntr, exist_ok=True)\n",
    "            os.makedirs(path_cntC, exist_ok=True)\n",
    "            \n",
    "            for i, img  in enumerate(processed):\n",
    "                for t, path in enumerate([path_cell, path_diap, path_cntr]):\n",
    "                    pimg = Image.fromarray(img[..., t])\n",
    "                    im_path = os.path.join(path, 'img_%03d.png'%i)\n",
    "                    pimg.save(im_path , quality=99)\n",
    "                    \n",
    "                cell_im = img[..., 0]\n",
    "                cntr_im = img[..., 2].copy()\n",
    "                mask_no_cell = cell_im <= 85\n",
    "                cntr_im[mask_no_cell] = 0\n",
    "        \n",
    "                pimg = Image.fromarray(cntr_im)\n",
    "                im_path = os.path.join(path_cntC, 'img_%03d.png'%i)\n",
    "                pimg.save(im_path , quality=99)\n",
    "                \n",
    "            del processed\n",
    "            del stack_norm\n",
    "                \n",
    "        t1 = timer()\n",
    "        print(t1 - t0, 's')\n",
    "        \n",
    "        # Tile merging\n",
    "        merged_tiles_ids = list(tiles_info.keys())\n",
    "        #gen lists\n",
    "        for merged_idx, ti in tiles_info.items():\n",
    "            ny, nx, tile_dx, tile_dy = ti[0]\n",
    "            tile_ids = ti[1]\n",
    "            \n",
    "            info_path = os.path.join(pcfg.datasets_path, '%03d' % merged_idx, 'tiles_info')\n",
    "            os.makedirs(info_path, exist_ok=True)\n",
    "            \n",
    "            lists = ['', '', '', '', '', '']\n",
    "            \n",
    "            for idx in tile_ids:\n",
    "                lists[0] += pcfg.datasets_path_proc + '%03d' % idx + r'\\imgs_aligned_all\\raw\\%03d.png'+'\\n'\n",
    "                lists[1] += pcfg.datasets_path_proc + '%03d' % idx + r'\\pred_cdc\\cell\\img_%03d.png'+'\\n'\n",
    "                lists[2] += pcfg.datasets_path_proc + '%03d' % idx + r'\\pred_cdc\\diap\\img_%03d.png'+'\\n'\n",
    "                lists[3] += pcfg.datasets_path_proc + '%03d' % idx + r'\\pred_cdc\\cntC\\img_%03d.png'+'\\n'\n",
    "                lists[4] += pcfg.datasets_path_proc + '%03d' % idx + r'\\imgs_aligned_all\\flr1\\%03d.png'+'\\n'\n",
    "                lists[5] += pcfg.datasets_path_proc + '%03d' % idx + r'\\imgs_aligned_all\\flr2\\%03d.png'+'\\n'\n",
    "                \n",
    "            tmap = '%d %d\\n' % (nx, ny)\n",
    "            idx = -1\n",
    "            for iy in range(ny):\n",
    "                for ix in range(nx):\n",
    "                    idx += 1\n",
    "                    \n",
    "                    tmap += '%d %d %d %d %d\\n' % (idx, ix, iy, ix * tile_dx, iy*tile_dy)\n",
    "                    \n",
    "            for tlist, fname in zip(lists, ['raw.tl', 'cell.tl', 'diap.tl', 'cntc.tl', 'flr1.tl', 'flr2.tl']):\n",
    "                fpath = os.path.join(info_path, fname)\n",
    "                with open(fpath, 'wt') as f:\n",
    "                    f.write(tlist)\n",
    "                    \n",
    "            fpath = os.path.join(info_path, 'map.tm')\n",
    "            with open(fpath, 'wt') as f:\n",
    "                f.write(tmap)\n",
    "                \n",
    "        # gen merging bat file\n",
    "        prog_tal = pcfg.proc_bin_path + 'proc_iv.bat'\n",
    "        \n",
    "        ds_id_to_idx = {i:idx for idx, i in enumerate(datasets_ids)}\n",
    "        \n",
    "        for idx in tiles_info:\n",
    "            cmd = '@echo off\\n'\n",
    "            cmd = 'pushd '+pcfg.proc_bin_path+'\\n'\n",
    "            \n",
    "            tile_idx = ds_id_to_idx[tiles_info[idx][1][0]]\n",
    "            n_tf = nums[tile_idx]\n",
    "            cmd += 'call ' + prog_tal + ' %03d' % idx  + ' %d' % n_tf + ' %d' % len(datasetsf_tmplts[tile_idx])\n",
    "            cmd += '\\n'\n",
    "            cmd += 'popd\\n'\n",
    "        \n",
    "            bat_file = pcfg.datasets_path + '%03d/'%idx + 'merge.bat'\n",
    "            with open(bat_file, 'wt') as f:\n",
    "                f.write(cmd)\n",
    "        \n",
    "        # gen all merging bat file\n",
    "        cmd = '@echo off\\n'\n",
    "        \n",
    "        for ds_idx in tiles_info:\n",
    "            cmd += 'call %03d'%ds_idx+'\\\\merge.bat \\n'\n",
    "        bat_file = pcfg.datasets_path + 'merge_' + str(merged_tiles_ids) + '.bat'\n",
    "        with open(bat_file, 'wt') as f:\n",
    "            f.write(cmd)\n",
    "            \n",
    "        if auto_proc:\n",
    "            start_remote_job(pcfg, bat_file)            \n",
    "        \n",
    "        # Cell segmentation\n",
    "        # gen segmentation bat file\n",
    "        prog_seg = pcfg.proc_bin_path + 'proc_ds_flr_n.bat'\n",
    "        \n",
    "        for i, idx in enumerate(datasets_ids+merged_tiles_ids):\n",
    "            test_idx = i if idx not in merged_tiles_ids else ds_id_to_idx[tiles_info[idx][1][0]]\n",
    "            has_flour = fluo_present[test_idx]\n",
    "            \n",
    "            cmd = '@echo off\\n'\n",
    "            cmd = 'pushd '+pcfg.proc_bin_path+'\\n'\n",
    "            cmd += 'call ' + prog_seg + ' %03d' % idx  + ' %d' % nums[test_idx]+ ' %d' % len(datasetsf_tmplts[test_idx])\n",
    "            cmd += '\\n'\n",
    "            cmd += 'popd\\n'\n",
    "        \n",
    "            bat_file = pcfg.datasets_path + '%03d/'%idx + 'segment.bat'\n",
    "            with open(bat_file, 'wt') as f:\n",
    "                f.write(cmd)\n",
    "                \n",
    "        # gen all segmentation bat file\n",
    "        bat_file_all_separate = gen_segm_bat(datasets_ids, pcfg)\n",
    "        bat_file_all_m = gen_segm_bat(merged_tiles_ids, pcfg)\n",
    "        bat_file_all_non_m = gen_segm_bat(all_non_tile_dataset_ids, pcfg)        \n",
    "        \n",
    "        # process tiles\n",
    "        # if auto_proc:\n",
    "        #     start_remote_job(pcfg, bat_file_all_separate)\n",
    "        \n",
    "        # wait merging and segmnet merged\n",
    "        if auto_proc:\n",
    "            start_remote_job(pcfg, bat_file_all_non_m)\n",
    "            \n",
    "        # wait merging and segmnet merged\n",
    "        if auto_proc:\n",
    "            start_remote_job(pcfg, bat_file_all_m)\n",
    "            \n",
    "        print(f'merged_tiles_ids: {merged_tiles_ids}, all_non_tile_dataset_ids: {all_non_tile_dataset_ids}')\n",
    "        \n",
    "        last_dataset_idx = max(merged_tiles_ids+all_non_tile_dataset_ids)\n",
    "        tgt_file = pcfg.datasets_path_proc + '%03d' % last_dataset_idx + r'\\segmentation\\cells\\tr_cells_tmp.dat'\n",
    "        wait_for_file(tgt_file)\n",
    "        \n",
    "        # End time report\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print(traceback.format_exc(), flush=True)\n",
    "        return proc_res\n",
    "    \n",
    "    nb_end_t = timer()\n",
    "    print(f'processing run time: {(nb_end_t - nb_start_t)/3600:.2f} h')\n",
    "\n",
    "    # result:\n",
    "    proc_res['status'] = True\n",
    "    proc_res['last_dataset_idx'] = last_dataset_idx\n",
    "    \n",
    "    return proc_res"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T08:23:01.978491Z",
     "start_time": "2024-11-01T08:23:01.876764Z"
    },
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7aaf9a114cd438fadf6f99c290bd621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<style>\\n    .widget-label { min-width: 200px !important; }\\n</style>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6bb0c473e2479592b9109e7a9f5dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='-1', description='Dataset range: first:', layout=Layout(width='300px"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19,
   "source": [
    "def seg_gui():\n",
    "    # 0. load config\n",
    "    proc_cfg = load_proc_cfg()\n",
    "    \n",
    "    # 1. \"datasets indexes range\" - 2 input fields for integers of first, last dataset idx\n",
    "    first_idx_guess = proc_cfg.next_dataset_id\n",
    "    last_idx_guess = first_idx_guess + 7\n",
    "    dataset_id_first_ti = Text(value=f'{first_idx_guess}', description='Dataset range: first:', layout=Layout(width='300px'))\n",
    "    dataset_id_last_ti = Text(value=f'{last_idx_guess}', description='last:', layout=Layout(width='300px'))\n",
    "    datasets_range_b = HBox([dataset_id_first_ti, dataset_id_last_ti]) \n",
    "    \n",
    "    # 2. \"dataset root path\" - input field for string with default value set to `datasets_path`\n",
    "    dataset_path_ts = Text(value=proc_cfg.datasets_path, description='Dataset root path:', layout=Layout(width='600px'))\n",
    "    \n",
    "    # 3. \"dataset proc root path\" - input field for string with default value set to `datasets_path_proc`\n",
    "    dataset_proc_path_ts = Text(value=proc_cfg.datasets_path_proc, description='Dataset proc root path:', layout=Layout(width='600px'))\n",
    "    \n",
    "    # 4. \"VovoFollow root path\" input field for string with default value set to `VF_root`\n",
    "    VF_root_ts = Text(value=proc_cfg.VF_root, description='VivoFollow root path:', layout=Layout(width='600px'))\n",
    "    \n",
    "    # 5. \"models path\" - input field for string with default value set to `models_path`\n",
    "    models_path_ts = Text(value=proc_cfg.models_path, description='Models path:', layout=Layout(width='600px'))\n",
    "    \n",
    "    # 6. \"reference datasets path\" - input field for string with default value set to `ref_ds_path`\n",
    "    ref_ds_path_ts = Text(value=proc_cfg.ref_ds_path, description='Reference datasets path:', layout=Layout(width='600px'))\n",
    "    \n",
    "    # 7. \"CUDA processing devices\" - input field for list of integers with default value set to `[0]`\n",
    "    dev_ids_str = ', '.join([str(i) for i in proc_cfg.cuda_dev_ids])\n",
    "    dev_ids_ts = Text(value=dev_ids_str, description='CUDA processing devices:', layout=Layout(width='600px'))\n",
    "    \n",
    "    # 8. \"reference dataset ID\" - input field for integer with default value set to `7`\n",
    "    rds_id_ti = Text(value=f'{proc_cfg.rds_id}', description='Reference dataset ID:', layout=Layout(width='600px'))\n",
    "    \n",
    "    # 9. Button \"Run\" to start processing. which prints in the output field the values of parameters\n",
    "    run_button = Button(description='Run', layout=Layout(width='600px'))\n",
    "    \n",
    "    # output\n",
    "    out = Output()\n",
    "    \n",
    "    def on_run_button_clicked(b, proc_cfg:ProcConfig):\n",
    "        proc_cfg.dataset_id_first_run = int(dataset_id_first_ti.value)\n",
    "        proc_cfg.dataset_id_last_run = int(dataset_id_last_ti.value)\n",
    "        proc_cfg.next_dataset_id = int(dataset_id_last_ti.value) + 1  # tentative - to be updated dependin\n",
    "        proc_cfg.datasets_path=dataset_path_ts.value\n",
    "        proc_cfg.models_path=models_path_ts.value\n",
    "        proc_cfg.ref_ds_path=ref_ds_path_ts.value\n",
    "        proc_cfg.datasets_path_proc=dataset_proc_path_ts.value\n",
    "        proc_cfg.VF_root=VF_root_ts.value\n",
    "        proc_cfg.rds_id=int(rds_id_ti.value)\n",
    "        proc_cfg.cuda_dev_ids = [int(i) for i in dev_ids_ts.value.replace(' ', '').replace('\\t', '').split(',')]\n",
    "        \n",
    "        with out:\n",
    "            print(dataclasses.asdict(proc_cfg))\n",
    "    \n",
    "        \n",
    "            proc_res = run_segmentation(proc_cfg)\n",
    "            \n",
    "            res = proc_res['status']\n",
    "        \n",
    "            if res:\n",
    "                print('Processing finished successfully')\n",
    "            else:\n",
    "                print('Processing failed. Please Contact support')\n",
    "                \n",
    "            last_dataset_idx = proc_res['last_dataset_idx']\n",
    "        \n",
    "            proc_cfg.next_dataset_id = last_dataset_idx\n",
    "        save_proc_cfg(proc_cfg)\n",
    "        \n",
    "    \n",
    "    on_run_button_clicked_p = partial(on_run_button_clicked, proc_cfg=proc_cfg)\n",
    "    run_button.on_click(on_run_button_clicked_p)\n",
    "    \n",
    "    # display\n",
    "    controls_b = VBox(\n",
    "        [\n",
    "            datasets_range_b,\n",
    "            dataset_path_ts,\n",
    "            dataset_proc_path_ts,\n",
    "            VF_root_ts,\n",
    "            models_path_ts,\n",
    "            ref_ds_path_ts,\n",
    "            dev_ids_ts,\n",
    "            rds_id_ti,\n",
    "            run_button,\n",
    "            out])\n",
    "    \n",
    "    display(HTML('''\n",
    "    <style>\n",
    "        .widget-label { min-width: 200px !important; }\n",
    "    </style>'''))\n",
    "    \n",
    "    display(controls_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "seg_gui()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
